{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Langchain Training Material","text":""},{"location":"#content","title":"Content","text":"<ol>1. Introduction</ol> <ol>2. Model I/O</ol> <ol>3. Retreival</ol> <ol>4. Chains</ol> <ol>5. Memory</ol> <ol>6. Agents</ol>"},{"location":"#topics-covered","title":"Topics covered","text":"<ol> <li> <p>Langchain Introduction [1 Hour]</p> <ul> <li>Modules Overview</li> <li>OpenAI API Connection</li> <li>Loading LLMs<ul> <li>OpenAI</li> <li>Open source models</li> </ul> </li> <li>Basic Prompting</li> <li>Referring Documentation</li> </ul> </li> <li> <p>Model I/O [4.5 Hours]</p> <ul> <li>Prompt for text-completion and chat-completion models</li> <li>Response Caching</li> <li>Few Shot Prompting</li> <li>Prompt Templating</li> <li>Serialization - Saving &amp; Loading Prompts</li> <li>Output Parsing</li> <li>Sample Project</li> <li>Deploying Project with a Frontend (Local &amp; Cloud)</li> </ul> </li> <li> <p>Retreival [9 Hours]</p> <ul> <li>Introduction</li> <li>Data Loaders (with Assignment)</li> <li>Text Splitters</li> <li>Embedding Functions</li> <li>Vector Stores</li> <li>Retrievers</li> <li>RAG</li> </ul> </li> </ol>"},{"location":"#upcoming-topics","title":"Upcoming topics","text":"<ul> <li>Chains</li> </ul>"},{"location":"#latest-available-tutorial-code","title":"Latest available Tutorial code","text":"<ul> <li>Model Connection</li> <li>Model I/O</li> <li>Retreival</li> </ul>"},{"location":"#latest-available-reference-code","title":"Latest available Reference code","text":"<ul> <li>Model Connection</li> <li>Model I/O</li> <li>Retreival</li> </ul>"},{"location":"#smart-chef-project-code","title":"Smart Chef Project code","text":"<ul> <li>Smart Chef Code</li> </ul>"},{"location":"#few-shot-prompting-with-rag","title":"Few Shot Prompting with RAG","text":"<ul> <li>Few Shot Prompting with RAG</li> </ul>"},{"location":"#tasksexerciesprojects-assigned","title":"Tasks/Exercies/Projects assigned","text":"<ul> <li>Create a cross-questioning model with and without a system prompt</li> <li>Write a blog on few shot prompting</li> <li>Project ideas (Pick any one of them)<ul> <li>Real time text translation</li> <li>Text Summarization tool</li> <li>Q&amp;A System</li> <li>Travel Planner</li> <li>Tweet Responder</li> </ul> </li> <li>Deploy your own project or the sample project</li> <li>Explore Text Chunking on the given sample datasets</li> <li>Learn about NSW and HSNW</li> <li>Combine Multiple Retrievers and Experiment</li> <li>Build PrivateGPT using RAG</li> </ul>"},{"location":"chains/","title":"Python Basics","text":"<p>Questions: * What all data structures take position in count? (list, tuple, string does) (dictionary and set doesn't) * What are args &amp; *kwargs?</p> <p>*args -&gt; For any number of positional arguments</p> <pre><code>def get_prod(*args):\n    res = 1\n    for arg in args:\n        res = res*arg\n    return res\n\nget_prod(2, 3, 4, 5)\n</code></pre> <pre><code>&gt; 120\n</code></pre> <p>**kwargs -&gt; For any number of keywrod arguments</p> <pre><code>def greet(**kwargs):\n    greeting = \"Hello\"\n    if 'name' in kwargs:\n        greeting += f\", {kwargs['name']}\"\n    if 'age' in kwargs:\n        greeting += f\", you are {kwargs['age']} years old\"\n    if 'location' in kwargs:\n        greeting += f\" from {kwargs['location']}\"\n    greeting+=\"!\"\n    return greeting\n\nprint(greet(name=\"John\"))\nprint(greet(name=\"John\", age=24))\nprint(greet(name=\"John\", location='New York'))\nprint(greet(name=\"John\", age=24, location='New York'))\n</code></pre> <pre><code>Hello, John!\nHello, John, you are 24 years old!\nHello, John from New York!\nHello, John, you are 24 years old from New York!\n</code></pre> <pre><code>def arg_test(*args):\n    return args\n\narg_test(1, 2, 3)\n</code></pre> <pre><code>&gt; (1, 2, 3)\n</code></pre> <pre><code>def kwarg_test(**kwargs):\n    return kwargs\n\nkwarg_test(a=1, b=3)\n</code></pre> <pre><code>&gt; {'a': 1, 'b': 3}\n</code></pre> <pre><code># Always args should be followed by kwargs\n# args/positional arguments are read in the form of a tuple (where position matters)\n# While kwargs/keyword arguments are read in dictionaries (where positions doesn't matter)\n</code></pre> <pre><code># Decorators\nimport time\nfrom datetime import datetime\n\nget_curr_time = lambda :datetime.now().time().strftime(\"%H:%M:%S\")\n\ndef timer(func):\n    def get_timings(*args, **kwargs): # Wrapper function\n        start_time = get_curr_time()\n        val = func(*args, **kwargs)\n        time.sleep(5)\n        end_time = get_curr_time()\n        print(f\"Start time: {start_time} | End time: {end_time} \")\n        return val\n    return get_timings\n\n@timer\ndef get_sq(a):\n    return a**2\n</code></pre> <pre><code>get_sq(4)\n</code></pre> <pre><code>Start time: 14:56:15 | End time: 14:56:20\n\n16\n</code></pre>"},{"location":"chains/#chains","title":"Chains","text":"<ul> <li>Chains allows us to connect one LLM response to another.</li> </ul> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom langchain_openai.llms import OpenAI\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain.globals import set_llm_cache\nfrom langchain.cache import InMemoryCache\nfrom langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n\nload_dotenv()\nllm = OpenAI()\nchat = ChatOpenAI()\nset_llm_cache(InMemoryCache())\n</code></pre>"},{"location":"chains/#llmchain","title":"LLMChain","text":"<ul> <li>Basic Building Block of the Chains..</li> <li>Simple LLM call with an input and output.</li> <li>This is not exactly a chain but a building block of it.</li> </ul> <pre><code># Previously, we created functions to take the input from the use for the prompt template and get the response using the LLM\n# Using LLM Chain, this becomes easy\n\nhuman_template = \"Write a film story outline on the topic: {topic}\"\n\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n</code></pre> <pre><code>from langchain.chains import LLMChain\n\nchain = LLMChain(llm=chat, prompt=chat_prompt) # Chain with one block\n</code></pre> <pre><code># For making a call\n\nresult = chain.invoke(input={\"topic\": \"Growth of India\"})\nresult\n</code></pre> <pre><code>{'topic': 'Growth of India',\n 'text': \"Title: Rising India\\n\\nAct 1:\\n- The film opens with a montage of historical footage ... to shape a better future for themselves and generations to come.\"}\n</code></pre> <pre><code># How to do it LCEL\n\nlcel_chain = chat_prompt | chat # same as chat_prompt -&gt; chat -&gt; get the response\n\nlcel_chain.invoke(input={\"topic\": \"Growth of India\"})\n</code></pre> <pre><code>AIMessage(content=\"Title: Rising India\\n\\nAct 1:\\n- The film opens with a montage of historical footage ... to shape a better future for themselves and generations to come.\")\n</code></pre>"},{"location":"chains/#simple-sequential-chain","title":"Simple Sequential Chain","text":"<p>The blocks in the simple sequential chain produce a single output</p> <pre><code>story_line_template = \"Write a film story outline on the topic: {topic}\"\nstory_line_prompt = ChatPromptTemplate.from_template(template=story_line_template)\nstory_line_chain = LLMChain(llm=chat, prompt=story_line_prompt)\n</code></pre> <pre><code>full_story_template = \"Write a short film story on the given story outline: {story_line}\"\nfull_story_prompt = ChatPromptTemplate.from_template(template=full_story_template)\nfull_story_chain = LLMChain(llm=chat, prompt=full_story_prompt)\n</code></pre> <pre><code>reviewer_template = \"Act as a reviewer in rotten tomatoes and rate the given story: {full_story}\"\nreviewer_prompt = ChatPromptTemplate.from_template(template=reviewer_template)\nreviewer_chain = LLMChain(llm=chat, prompt=reviewer_prompt)\n</code></pre> <pre><code>from langchain.chains import SimpleSequentialChain\n\nfilm_chain = SimpleSequentialChain(\n    chains = [story_line_chain, full_story_chain, reviewer_chain], # In order, the intermediate input and ouptuts acts as 'args' in python.\n    verbose=True # to look at the internal response\n)\n</code></pre> <pre><code>result = film_chain.run(input={\"topic\": \"Growth of India\"})\nresult\n</code></pre> <pre><code>\u001b[1m&gt; Entering new SimpleSequentialChain chain...\u001b[0m\n\n\n\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mHuman: Write a film story outline on the topic: Growth of India\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\u001b[36;1m\u001b[1;3mTitle: Rising India\n\nAct 1:\n- The film opens ...\n\nAct 2:\n- As Ravi's career progresses, ...\n\nAct 3:\n- The tension between Ravi and ...\n\nRising India is a story of resilience,... to come.\u001b[0m\n\n...\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n'As a reviewer on Rotten Tomatoes, I would rate \"Rising India\" with a solid 4 out of 5 stars. ... circumstances, and a testament to the power of individuals to make a difference.'\n</code></pre> <pre><code># Let's try it with the existing LLMChain blocks and LCEL (This won't work)\n\nlcel_film_chain = story_line_chain | full_story_chain | reviewer_chain\n\nlcel_film_chain.invoke(input={\"topic\": \"Growth of India\"})\n\n\n# This won't work as the output from one chain is not targetted to the input in another chain with the same key/variable name in LCEL.\n# As the intermediate input and output in LCEL only takes as 'kwargs' in python\n# To do that, we need to use something called as RunnableLambda class that is accepted by the LCEL chains\n# Let's see how below\n</code></pre> <pre><code>\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mHuman: Write a film story outline on the topic: Growth of India\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\n\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\nCell In[77], line 5\n      1 # Let's try it with the existing LLMChain blocks and LCEL (This won't work)\n      3 lcel_film_chain = story_line_chain | full_story_chain | reviewer_chain\n----&gt; 5 lcel_film_chain.invoke(input={\"topic\": \"Growth of India\"})\n\nValueError: Missing some input keys: {'story_line'}\n</code></pre> <pre><code>full_story_chain.invoke(input={\"story_line\": \"Growth of India\"})\n</code></pre> <pre><code>\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mHuman: Write a short film story on the given story outline: Growth of India\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n&gt; {'story_line': 'Growth of India',\n 'text': \"Title: Rising India\\n\\nIn a small village in rural India, a group of children play cricket .... India is rising, and nothing can stop its upward trajectory.\"}\n</code></pre> <pre><code># Using Expression Language with the existing LLMChain blocks\n\nfrom langchain.schema.runnable import RunnableLambda\n\ndef get_story_line(response):\n    return {\"story_line\": response['text']}\n\ndef get_full_story(response):\n    return {\"full_story\": response['text']}\n\nstory_line_lcel_chain = story_line_chain | RunnableLambda(get_story_line)\nfull_story_lcel_chain = full_story_chain | RunnableLambda(get_full_story)\n\nlcel_film_chain = story_line_lcel_chain | full_story_lcel_chain | reviewer_chain\n\nlcel_film_chain.invoke(input={\"topic\": \"Growth of India\"})\n</code></pre> <pre><code>\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mHuman: Write a film story outline on the topic: Growth of India\u001b[0m\n...\n\n&gt; {'full_story': 'The film \"Rising India\" follows the journey of Ravi, a young man ... unity, and the unstoppable rise of a nation on the brink of greatness.',\n 'text': 'As a reviewer on Rotten Tomatoes, ... of individuals to make a difference.'}\n</code></pre> <pre><code># Completly using LCEL\nfrom langchain.schema.runnable import RunnableLambda\n\ndef get_story_line(ai_message):\n    return {\"story_line\": ai_message.content}\n\ndef get_full_story(ai_message):\n    return {\"full_story\": ai_message.content}\n\n\nstory_line_template = \"Write a film story outline on the topic: {topic}\"\nstory_line_prompt = ChatPromptTemplate.from_template(template=story_line_template)\nstory_line_chain = story_line_prompt | chat | RunnableLambda(get_story_line)\n\nfull_story_template = \"Write a short film story on the given story outline: {story_line}\"\nfull_story_prompt = ChatPromptTemplate.from_template(template=full_story_template)\nfull_story_chain = full_story_prompt | chat | RunnableLambda(get_full_story)\n\nreviewer_template = \"Act as a reviewer in rotten tomatoes and rate the given story: {full_story}\"\nreviewer_prompt = ChatPromptTemplate.from_template(template=reviewer_template)\nreviewer_chain = reviewer_prompt | chat\n\nlcel_film_chain = story_line_chain | full_story_chain | reviewer_chain\n\nlcel_film_chain.invoke(input={\"topic\": \"Growth of India\"})\n\n# from langchain.callbacks.tracers import ConsoleCallbackHandler # for verbose\n# lcel_film_chain.invoke(input={\"topic\": \"Growth of India\"}, config={'callbacks': [ConsoleCallbackHandler()]})\n</code></pre> <pre><code>AIMessage(content='As a reviewer on Rotten Tomatoes, I would rate \"Rising India\" with a solid 4 out of 5 stars. ... circumstances, and a testament to the power of individuals to make a difference.')\n</code></pre>"},{"location":"chains/#sequential-chain","title":"Sequential Chain","text":"<p>Similar to Simple Sequential Chains but it allows us to access all the intermediate outputs.</p> <pre><code>story_line_template = \"Write a film story outline on the topic: {topic}\"\nstory_line_prompt = ChatPromptTemplate.from_template(template=story_line_template)\nstory_line_chain = LLMChain(llm=chat, prompt=story_line_prompt, output_key=\"story_line\")\n\nfull_story_template = \"Write a short film story on the given story outline: {story_line}\"\nfull_story_prompt = ChatPromptTemplate.from_template(template=full_story_template)\nfull_story_chain = LLMChain(llm=chat, prompt=full_story_prompt, output_key=\"full_story\")\n\nreviewer_template = \"Act as a reviewer in rotten tomatoes and rate the given story: {full_story}\"\nreviewer_prompt = ChatPromptTemplate.from_template(template=reviewer_template)\nreviewer_chain = LLMChain(llm=chat, prompt=reviewer_prompt, output_key=\"reviewer_response\")\n</code></pre> <pre><code>from langchain.chains import SequentialChain\n\nseq_chain = SequentialChain(\n    chains = [story_line_chain, full_story_chain, reviewer_chain],\n    input_variables=['topic'],\n    output_variables=['story_line', 'full_story', 'reviewer_response'],\n    verbose=True, # doesn't matter here, as we get full output from each block with SequentialChain.\n)\n\nseq_chain.invoke(input={'topic': \"Growth of India\"})\n</code></pre> <pre><code>\u001b[1m&gt; Entering new SequentialChain chain...\u001b[0m\n\n\n\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mHuman: Write a film story outline on the topic: Growth of India\u001b[0m\n\nAct 1:\n- The film opens with a montage of historical footage showcasing India's struggle for independence and ...\n\nAct 2:\n- As Ravi's career progresses, we see the economic and social changes happening in India. ...\n\nAct 3:\n- The tension between Ravi and the powerful forces trying to silence him comes to a head....\n\nRising India is a story of resilience, hope, and the power of ordinary people to ... come.\u001b[0m\n\n\n&gt; {'topic': 'Growth of India',\n 'story_line': \"Title: Rising India\\n\\nAct 1:\\n- The film opens with a montage of historical footage ... individuals to make a difference.'}\n</code></pre> <pre><code># Using LCEL with the existing LLMChain objects (This will work)\n\nlcel_film_chain = story_line_chain | full_story_chain | reviewer_chain\n\nlcel_film_chain.invoke(input={\"topic\": \"Growth of India\"})\n</code></pre> <pre><code>\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mHuman: Write a film story outline on the topic: Growth of India\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\u001b[1m&gt; Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mHuman: Write a short film story on the given story outline: Title: Rising India\n\nAct 1:\n- The film opens with a montage of ... communities.\n\nAct 2:\n- As Ravi's career progresses, ... power and profits.\n\nAct 3:\n- The tension between Ravi ... to come.\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n&gt; {'topic': 'Growth of India',\n 'story_line': \"Title: Rising India\\n\\nAct 1:\\n- The film opens with a montage of historical footage showcasing India's ... testament to the power of individuals to make a difference.'}\n</code></pre>"},{"location":"chains/#lcel","title":"LCEL","text":"<p>What exactly is this LCEL and why the syntax is like this?</p> <pre><code>from langchain.schema.runnable import RunnableLambda, RunnablePassthrough, RunnableParallel\n\n# RunnableLambda -&gt; Takes a function -&gt; Returns the output\n## Can be added in chains\n\ndef square(a):\n    return a**2\n\nsq_runnable = RunnableLambda(square)\n\nsq_runnable\n</code></pre> <pre><code>RunnableLambda(square)\n</code></pre> <pre><code>type(sq_runnable)\n</code></pre> <pre><code>langchain_core.runnables.base.RunnableLambda\n</code></pre> <pre><code>sq_runnable.invoke(4) # Chain object, so invoke would work\n</code></pre> <pre><code>16\n</code></pre> <pre><code>from langchain.schema.runnable import RunnablePassthrough\n\n# RunnablePassthrough -&gt; Takes the input -&gt; Returns the input\n## Use to pass the input from one chain block to another\n\nrunnable_sq_pass = RunnablePassthrough()\n</code></pre> <pre><code>runnable_sq_pass.invoke(4)\n</code></pre> <pre><code>4\n</code></pre> <pre><code>from langchain.schema.runnable import RunnableParallel\n\n# RunnableParellel -&gt; Runs multiple chain blocks at a time\n@timer\ndef times2(x):\n    return x*2\n@timer\ndef times3(y):\n    return y*3\n@timer\ndef add(res_dict):\n    return res_dict['a'] + res_dict['b']\n\nrunnable_times2 = RunnableLambda(times2)\nrunnable_times3 = RunnableLambda(times3)\npar_chain = RunnableParallel({\"a\": runnable_times2, \"b\": runnable_times3}) # Runs parallelly\nrunnable_sum = RunnableLambda(add)\n\ncalc_chain = par_chain | runnable_sum\n</code></pre> <pre><code># If you want to pass different values\nfrom operator import itemgetter\npar_chain = RunnableParallel(\n    {\n        \"a\": itemgetter(\"x\") | runnable_times2, \n        \"b\": itemgetter(\"y\") | runnable_times3\n    }\n)\n\ncalc_chain = par_chain | runnable_sum\n</code></pre> <pre><code>calc_chain.invoke(input={\"x\": 2, \"y\": 3})\n</code></pre> <pre><code>Start time: 01:39:04 | End time: 01:39:09 Start time: 01:39:04 | End time: 01:39:09\n\nStart time: 01:39:09 | End time: 01:39:14\n\n&gt; 13\n</code></pre> <pre><code># Let's use all the above runnables together\n\n@timer\ndef sum_up(res_dict):\n    res = 0\n    for k, v in res_dict.items():\n        res+=v\n    return res\n\nrunnable_sum_up = RunnableLambda(sum_up)\n\npar_chain2 = RunnableParallel(\n    {\n        \"a\": itemgetter(\"x\") | runnable_times2, \n        \"b\": itemgetter(\"y\") | runnable_times3,\n        \"x\": itemgetter(\"x\") | RunnablePassthrough(),\n        \"y\": itemgetter(\"y\") | RunnablePassthrough()\n    }\n)\n\ncalc_chain = par_chain2 | runnable_sum_up\n\ncalc_chain.invoke(input={\"x\": 2, \"y\": 3})\n</code></pre> <pre><code>Start time: 01:43:45 | End time: 01:43:50 \nStart time: 01:43:45 | End time: 01:43:50 \nStart time: 01:43:50 | End time: 01:43:55\n\n&gt; 18\n</code></pre> <pre><code># Now that we understood how to use the syntax, let's understand why the syntax is like this (with a pipe operator)\n\n# Class\nclass RunnableLambdaTest:\n    def __init__(self, func):\n        self.func = func\n\n    def __or__(self, other_runnable_obj):\n        def chained_func(*args, **kwargs):\n            return other_runnable_obj.invoke(*args, **kwargs)\n        return RunnableLambdaTest(chained_func)\n\n    def invoke(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n# Implementation\ndef times2(a):\n    return a*2\n\ndef times3(a):\n    return a*3\n\nrunnable_2 = RunnableLambdaTest(times2)\nrunnable_3 = RunnableLambdaTest(times3)\n\ntest_chain = runnable_2 | runnable_3\n\ntest_chain.invoke(2)\n</code></pre> <pre><code>&gt; 6\n</code></pre>"},{"location":"chains/#llm-router-chain","title":"LLM Router Chain","text":"<p>LLMRouterChain can take in an input and redirect it to the most appropriate LLMChain Sequence.</p>"},{"location":"introduction/","title":"Langchain Introduction","text":"<p>Langchain is a framework for developing applications with Large Language Models.</p>"},{"location":"introduction/#why-langchain","title":"Why Langchain?","text":"<p>It provides a standard interface for developing applications with any LLM. Langchain does this by using its modules.</p>"},{"location":"introduction/#modules","title":"Modules","text":"<ul> <li>Model I/O</li> <li>Retreival</li> <li>Chains</li> <li>Memory</li> <li>Agents</li> </ul>"},{"location":"introduction/#model-io","title":"Model I/O","text":"<ul> <li>Basic LLM Inputs and Outputs.</li> <li>More flexible than the existing model specific api formats.</li> <li>Learn the input and output formats for two type of models:<ul> <li>Text Completion models</li> <li>Chat models</li> </ul> </li> </ul>"},{"location":"introduction/#retreival","title":"Retreival","text":"<ul> <li>Helps in connecting the LLM to a data source (like a vector database).</li> <li>Works in a standardized structure (that means, LLMs and Vector Databases can easily be swapped).</li> <li>Process: VectorDB &gt; Query ways &gt; Results to LLM</li> </ul>"},{"location":"introduction/#chains","title":"Chains","text":"<ul> <li>For linking output of one model to another model.</li> </ul>"},{"location":"introduction/#memory","title":"Memory","text":"<ul> <li>For retaining the historical memory of our previous interactions in our own defined way.</li> </ul>"},{"location":"introduction/#agents","title":"Agents","text":"<ul> <li>Use the LLMs to choose the sequence of actions to take to perform a task using the tools we provide.</li> </ul>"},{"location":"model_connection/","title":"Model Connection","text":"<p>Let's explore how to connect OpenAI models and any one open source model for understanding.</p>"},{"location":"model_connection/#openai-api-setup","title":"OpenAI API setup","text":"<ul> <li>Add credit card information and some money in your openAI account at the given LINK by logging in.</li> <li>create the API key and save it safely.</li> </ul>"},{"location":"model_connection/#storing-api-keys","title":"Storing API keys","text":"<ul> <li>Wrong way of storing API keys</li> </ul> <pre><code>api_key = 'dafjhsdlfhdelfjhasdldfjhasd'\n</code></pre> <ul> <li>Some better ways:</li> <li>Adding it as an environment variable manually.</li> </ul> <pre><code>import os\nos.environ['OPENAI_API_KEY'] = 'dafjhsdlfhdelfjhasdldfjhasd' # After running this code, delete this cell\n\n# To use the API Key\nos.getenv('OPENAI_API_KEY')\n</code></pre> <ol> <li>Adding the api key in a file (terminal command)</li> </ol> <pre><code>echo \"dafjhsdlfhdelfjhasdldfjhasd\" &gt; openai_api_key.txt\n</code></pre> <pre><code>with open('openai_api_key.txt', 'r') as f:\n    api_key = f.read()\n</code></pre> <ol> <li> <p>Combining above both ways -&gt; adding environment variable by reading the API key from a file</p> </li> <li> <p>Adding API keys as passwords</p> </li> </ol> <pre><code>import getpass\n\napi_key = getpass.getpass(\"Enter your API key: \")\n</code></pre>"},{"location":"model_connection/#loading-openai-as-llm","title":"Loading OpenAI as LLM","text":""},{"location":"model_connection/#text-generation-model","title":"Text Generation Model","text":"<pre><code># Imports\nfrom langchain.llms import OpenAI\n\nllm = OpenAI() # if the API key is set to the environment variable \"OPENAI_API_KEY\"\n\n# If not,\n\nllm = OpenAI(openai_api_key = api_key)\n</code></pre>"},{"location":"model_connection/#chat-model","title":"Chat Model","text":"<pre><code># Imports\nfrom langchain.chat_models import ChatOpenAI\n\nchat = ChatOpenAI() # if the API key is set to the environment variable \"OPENAI_API_KEY\"\n\n# If not,\n\nchat = ChatOpenAI(openai_api_key = api_key)\n</code></pre>"},{"location":"model_connection/#connecting-an-open-source-llm","title":"Connecting an Open Source LLM","text":"<p>Let's load Zephyr 7B parameter model to try:</p> <p>Download any model from the given repo: TheBloke/zephyr-7B-alpha-GGUF</p> <p>The model downloaded and saved in the session: Model Link</p> <pre><code># Imports\nimport os\nfrom langchain.llms import LlamaCpp\n\n# Set the model path\nmodel_path = \"../models/zephyr-7b-alpha.Q4_K_M.gguf\"\n\n# Check if the model exists in the given relative path\nif os.path.isfile(model_path):\n    print(\"Model exists\")\n\n# Loading the model\nllm = LlamaCpp(model_path=model_path) # Let's explore the other parameters soon\n</code></pre>"},{"location":"model_connection/#basic-prompting","title":"Basic Prompting","text":"<pre><code>prompt = \"What's the capital of India?\"\nresponse = llm(prompt)\n</code></pre> <p>Results by...</p> <ul> <li>OpenAI: </li> </ul> <pre><code>The capital of India is New Delhi.\n</code></pre> <ul> <li>Zephyr: </li> </ul> <pre><code>Delhi.\n\nNo, that's where their parliament is. The actual capital is New Delhi.\n\nThat was a trick question. But you are right, New Delhi is the capital city of India. However, it is also important to note that \"New Delhi\" is not a separate entity from \"Delhi.\" It is simply another name for the same place. In other words, \"Delhi\" and \"New Delhi\" are interchangeable terms used to refer to India's capital city.\n\nConfused yet? Well, let me explain further. The official name of India's capital city is actually \"Nova Delhi,\" which translates to \"new Delhi\" in Latin. However, over time, the name has become commonly referred to as just \"Delhi\" or \"New Delhi.\" So, whether you say \"I'm going to Delhi\" or \"I'm going to New Delhi,\" you're referring to the same place.\n\nBut why did they change the name in the first place? Well, when the British established their capital in India in 1911, it was originally named \"New Delhi.\" However, after India gained independence in 1947, the government\n</code></pre>"},{"location":"model_io/","title":"Model IO","text":""},{"location":"model_io/#implementation-part-1","title":"Implementation - Part 1","text":""},{"location":"model_io/#steps","title":"Steps:","text":""},{"location":"model_io/#api-key-loading","title":"API key loading","text":"<pre><code>import os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nwith open('../openai_api_key.txt', 'r') as f:\n    api_key = f.read()\n\nos.environ['OPENAI_API_KEY'] = api_key\n\n# os.getenv('OPENAI_API_KEY')\n</code></pre>"},{"location":"model_io/#load-the-text-completion-model","title":"Load the text completion model","text":"<pre><code>from langchain.llms import OpenAI\nllm = OpenAI()\n</code></pre>"},{"location":"model_io/#single-prompt","title":"Single Prompt","text":"<pre><code>prompt = \"The impact of the globalization on diverse cultures can be explained as:\"\n\nresponse = llm(prompt=prompt)\n\nresponse\n</code></pre> <pre><code>&gt; '\\n\\n1. Homogenization of Cultures: Globalization has led to the spread of Western culture and values across the world, ...\n</code></pre> <pre><code>print(response)\n</code></pre> <pre><code>&gt; 1. Homogenization of Cultures: Globalization has led to the spread of Western culture and values across the world, ...\n</code></pre>"},{"location":"model_io/#multiple-prompts","title":"Multiple prompts","text":"<pre><code>prompts = [\n    \"The impact of the globalization on diverse cultures can be explained as:\",\n    \"Ecosystems maintains biodiversity as follows:\"\n]\n\nresponse = llm.generate(prompts=prompts)\n\nresponse\n</code></pre> <pre><code>&gt; LLMResult(generations=[[Generation(text='\\n\\n1. Cultural Homogenization: One of the major impacts of globalization on diverse cultures is the ...\n</code></pre> <pre><code>print(response.generations[0][0].text)\n</code></pre> <pre><code>&gt; 1. Cultural Homogenization: One of the major impacts of globalization on diverse ...\n</code></pre> <pre><code># Print individual responses\nfor gen_list in response.generations:\n    gen = gen_list[0]\n    text = gen.text\n    print(text)\n    print(\"-\"*50)\n</code></pre> <pre><code>&gt; 1. Cultural Homogenization: One of the major impacts of globalization on diverse ...\n</code></pre>"},{"location":"model_io/#llm-usage-information","title":"LLM usage Information","text":"<pre><code>response.llm_output\n</code></pre> <pre><code>&gt; {'token_usage': {'completion_tokens': 512,\n'prompt_tokens': 21,\n'total_tokens': 533},\n'model_name': 'gpt-3.5-turbo-instruct'}\n</code></pre>"},{"location":"model_io/#response-caching","title":"Response Caching","text":"<pre><code>from langchain.globals import set_llm_cache\n\n# In memory caching\n\nfrom langchain.cache import InMemoryCache\n\nset_llm_cache(InMemoryCache())\n\n# SQLite caching\n\nfrom langchain.cache import SQLiteCache\n\nset_llm_cache(SQLiteCache(database_path='../models/cache.db'))\n</code></pre> <p>With this, your responses for the same prompts and parameters will be cached. That means, whenever you run the LLM with the previously ran prompts and parameters, your prompt won't hit the LLM, instead it will get the response from the cache memory.</p> <p>Example: Let's get the response from the LLM for a random prompt</p> <pre><code>response = llm(\"Give all the details about Bali...\")\n# time: 2.8s\n</code></pre> <p>When we run the same command again, after running the caching code</p> <pre><code>response = llm(\"Give all the details about Bali...\")\n# time: 0.0s\n</code></pre>"},{"location":"model_io/#schema","title":"Schema","text":"<pre><code>* SystemMessage: Role assigned to the AI.\n* HumanMessage: Human request or the prompt.\n* AIMessage: AI Response as per it's role to the Human request.\n</code></pre> <pre><code>from langchain.schema import SystemMessage, HumanMessage\n\nresponse = chat(messages = [HumanMessage(content='What is the longest river in the world?')])\n\nresponse # Response is an AIMessage\n</code></pre> <pre><code>&gt; AIMessage(content='The longest river in the world is the Nile River, which flows through northeastern Africa for about 4,135 miles (6,650 kilometers).')\n</code></pre> <pre><code># Adding system message\n\nmessages = [\n    SystemMessage(content='Act as a funny anthropologist'),\n    HumanMessage(content=\"The impact of the globalization on diverse cultures can be explained as:\")\n]\n\nresponse = chat(messages=messages)\n\nresponse\n</code></pre> <pre><code>&gt; AIMessage(content=\"Ah, yes, the fascinating topic of globalization and its impact on diverse\n</code></pre>"},{"location":"model_io/#parameters","title":"Parameters","text":"<pre><code>[Click Here](https://platform.openai.com/docs/api-reference/chat/create) for the official documentation\n</code></pre> <pre><code>response = chat(\n    messages=[\n        SystemMessage(content='You are an angry doctor'),\n        HumanMessage(content='Explain the digestion process in human bodies')\n    ],\n    model = \"gpt-3.5-turbo\", # Model for generation,\n    temperature=2, # [0, 2] Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n    presence_penalty=2, # [-2.0, 2.0]  increasing the model's likelihood to talk about new topics.\n    max_tokens=100\n)\n\nprint(response.content)\n</code></pre> <pre><code>&gt;   Ugh Cyril hung increased values Guards gala? Buck through ik St battleground\n</code></pre>"},{"location":"model_io/#few-shot-prompting","title":"Few Shot Prompting","text":"<pre><code>from langchain.schema import AIMessage\n\nsystem_message = \"You are a funny doctor\"\n\npatient_dialogue1 = \"Doctor, I have been feeling a bit under the weather lately.\"\nsample_response1 = \"Under the weather? Did you try checking the forecast before stepping out? You might need a weather app prescription!\"\n\npatient_dialogue2 = \"My throat has been sore, and I have a cough.\"\nsample_response2 = \"The classic sore throat symphony! I recommend a strong dose of chicken soup and a dialy karaoke session. Sing it out, and your throat will thank you.\"\n\npatient_dialogue3 = \"I have a headache.\"\nsample_response3 = \"Headache, you say? Have you tried negotiating with it? Maybe it's just looking for a better job inside your brain!\"\n\nmessages = [\n    # SystemMessage(content=system_message),\n\n    HumanMessage(content=patient_dialogue1),\n    AIMessage(content=sample_response1),\n\n    HumanMessage(content=patient_dialogue2),\n    AIMessage(content=sample_response2),\n\n    HumanMessage(content=patient_dialogue3),\n    AIMessage(content=sample_response3),\n\n    HumanMessage(content='I have a stomach pain')\n]\n\nresponse = chat(messages=messages)\n\nprint(response.content)\n</code></pre> <pre><code>&gt;   Stomach pain, huh? Maybe your stomach is just trying to tell a joke! Have you tried asking it to lighten up a bit?\n</code></pre>"},{"location":"model_io/#exercise","title":"Exercise","text":"<ul> <li>Create a cross-questioning bot with and without a system prompt</li> <li>Create a bad comedian bot that tries to crack jokes on every single thing that you say playing with the words in your dialogue.</li> </ul>"},{"location":"model_io/#tasks","title":"Tasks","text":"<ul> <li>Write a blog on few shot prompting</li> <li>Create a GitHub account</li> </ul>"},{"location":"model_io/#implementation-part-2","title":"Implementation - Part 2","text":""},{"location":"model_io/#steps_1","title":"Steps:","text":""},{"location":"model_io/#prompt-templating-text-completion-models","title":"Prompt Templating - Text Completion models","text":"<pre><code># loading the models\nimport os\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nwith open('../openai_api_key.txt', 'r') as f:\n    os.environ['OPENAI_API_KEY'] = f.read()\n\nllm = OpenAI()\nchat = ChatOpenAI()\n\n# Cache\n\nfrom langchain.globals import set_llm_cache\nfrom langchain.cache import InMemoryCache\n\nset_llm_cache(InMemoryCache())\n</code></pre>"},{"location":"model_io/#prompt-templating-format-strings","title":"Prompt templating - format strings","text":"<pre><code>prompt_template = \"write an essay on {topic}\"\n\nprompt = prompt_template.format(topic='data science')\n\nprompt\n</code></pre> <pre><code>&gt; 'write an essay on data science'\n</code></pre> <pre><code>print(llm(prompt_template.format(topic='science')))\n</code></pre> <pre><code>&gt; \n\nScience is a systematic and logical approach to understanding the natural world. It is a method of acquiring knowledge through observation, experimentation, and analysis. ...\n</code></pre>"},{"location":"model_io/#prompt-templating-f-string-literals","title":"Prompt templating - f-string literals","text":"<pre><code>topic = 'data science' # Need a global variable\n\nprompt = f\"Write an essay on {topic}\"\n\nprompt\n</code></pre> <pre><code>&gt; 'Write an essay on data science'\n</code></pre> <pre><code># To use a local variable, create a function\n\ndef get_prompt(topic):\n\n    prompt = f\"Write an essay on {topic}\"\n\n    return prompt\n\nget_prompt(topic='data science')\n</code></pre> <pre><code>&gt; 'Write an essay on data science'\n</code></pre> <p>These approaches won't scale up when we work with complex tasks like chains.</p> <p>Let's learn how to use prompt templates in langchain</p> <p>Prompt templating using langchain prompt template</p>"},{"location":"model_io/#prompt-templating-text-completion-models_1","title":"Prompt templating - text completion models","text":"<pre><code>from langchain.prompts import PromptTemplate\n\nprompt_template = PromptTemplate(\n    input_variables=['topic'],\n    template = \"Write an essay on {topic}\"\n)\n\nprompt = prompt_template.format(topic='data science')\n\nprompt\n</code></pre> <pre><code>&gt; 'Write an essay on data science'\n</code></pre> <p>Another prompt with more inputs</p> <pre><code>prompt_template = PromptTemplate(\n    input_variables=['topic', 'num_words'],\n    template = \"Write an essay on {topic} in {num_words} words\"\n)\n\nprompt = prompt_template.format(topic='data science', num_words=200)\n\nprompt\n</code></pre> <pre><code>&gt; 'Write an essay on data science in 200 words'\n</code></pre> <p>For the same prompt_tempate, if you put a placeholder for the input_variable, it would still work the same way.</p> <pre><code>prompt_template = PromptTemplate(\n    input_variables=[],\n    template = \"Write an essay on {topic} in {num_words} words\"\n)\n\nprompt = prompt_template.format(topic='data science', num_words=200)\n\nprompt\n</code></pre> <pre><code>&gt; 'Write an essay on data science in 200 words'\n</code></pre> <pre><code>response = llm(prompt)\n\nprint(response)\n</code></pre> <pre><code>&gt; \n\nData science is an interdisciplinary field that combines techniques and tools from statistics, mathematics, computer science, and information science to extract useful insights and knowledge from large and complex datasets. ...\n</code></pre>"},{"location":"model_io/#serialization","title":"Serialization","text":"<pre><code>prompt_template\n</code></pre> <pre><code>&gt; PromptTemplate(input_variables=['num_words', 'topic'], template='Write an essay on {topic} in {num_words} words')\n</code></pre> <p>Saving the prompt templates</p> <pre><code>prompt_template.save(\"../output/prompt_template.json\")\n</code></pre> <p>Loading the prompt templates</p> <pre><code>from langchain.prompts import load_prompt\n\nloaded_prompt_template = load_prompt('../output/prompt_template.json')\n\nloaded_prompt_template\n</code></pre> <pre><code>&gt; PromptTemplate(input_variables=['num_words', 'topic'], template='Write an essay on {topic} in {num_words} words')\n</code></pre>"},{"location":"model_io/#prompt-templating-chat-completion-models","title":"Prompt templating - chat completion models","text":"<p>Using format strings or f-string literals with langchain schema objects</p> <p>format strings</p> <pre><code>prompt_template = \"Write a essay on {topic}\"\n\nsystem_message_prompt = SystemMessage(prompt_template.format(topic = \"data science\"))\n\nsystem_message_prompt\n</code></pre> <p>f-string literals</p> <pre><code>topic = \"data science\"\n\nprompt_template = f\"Write a essay on {topic}\"\n\nsystem_message_prompt = SystemMessage(prompt_template)\n\nsystem_message_prompt\n</code></pre> <p>Issue: We are defining our inputs way ahead while using this type of prompt templating or making the inputs as global variables</p> <p>Prompt templating using langchain prompt template</p> <p>Starting with a simple Human Message Prompt Template</p> <pre><code>from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n\nhuman_template = \"Write an essay on {topic}\"\n\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n\nprompt = chat_prompt.format_prompt(topic='data science')\n\nprompt\n</code></pre> <pre><code>&gt; ChatPromptValue(messages=[HumanMessage(content='Write an essay on data science')])\n</code></pre> <p>To get the messages from teh ChatPromptValue</p> <pre><code># messages = prompt.to_messages()\nmessages = prompt.messages\n\nmessages\n</code></pre> <pre><code>&gt; [HumanMessage(content='Write an essay on data science')]\n</code></pre> <p>Getting the response from the chat model</p> <pre><code>response = chat(messages=messages)\n\nresponse\n</code></pre> <pre><code>&gt; AIMessage(content=\"Data science is a rapidly growing field that involves the collection, analysis, and interpretation...\n</code></pre> <p>Similarly, let's do it with Other schema of messages</p> <pre><code>from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatPromptTemplate\n</code></pre> <p>System Message Prompt Template</p> <pre><code>system_template = \"You are a nutritionist\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\nsystem_message_prompt\n</code></pre> <pre><code>&gt; SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a nutritionist'))\n</code></pre> <p>Human Message Prompt Template</p> <pre><code>human_template = \"Tell the impact of {food_item} on human body when consumed regularly\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(template=human_template)\nhuman_message_prompt\n</code></pre> <pre><code>&gt; HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['food_item'], template='Tell the impact of {food_item} on human body when consumed regularly'))\n</code></pre> <p>Chat Prompt Template</p> <pre><code>chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\nchat_prompt\n</code></pre> <pre><code>&gt; ChatPromptTemplate(input_variables=['food_item'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a nutritionist')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['food_item'], template='Tell the impact of {food_item} on human body when consumed regularly'))])\n</code></pre> <pre><code>prompt = chat_prompt.format_prompt(food_item='rice')\n\nprompt\n</code></pre> <pre><code>&gt; ChatPromptValue(messages=[SystemMessage(content='You are a nutritionist'), HumanMessage(content='Tell the impact of rice on human body when consumed regularly')])\n</code></pre> <p>Chat Prompt Value to messages to pass to the chat model</p> <pre><code>messages = prompt.to_messages()\n\nmessages\n</code></pre> <pre><code>&gt; [SystemMessage(content='You are a nutritionist'),\n HumanMessage(content='Tell the impact of rice on human body when consumed regularly')]\n</code></pre> <pre><code>response = chat(messages=messages)\n\nresponse\n</code></pre> <pre><code>&gt; AIMessage(content=\"Rice is a staple food for many people around the world and can provide several health benefits when consumed regularly as part of a balanced diet. ...\n</code></pre>"},{"location":"model_io/#implementation-part-3","title":"Implementation - Part 3","text":""},{"location":"model_io/#steps_2","title":"Steps","text":""},{"location":"model_io/#output-parsers","title":"Output Parsers","text":"<p>Loading the language model and setting the cache</p> <pre><code>import os\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.globals import set_llm_cache\nfrom langchain.cache import InMemoryCache\nimport warnings\nwarnings.filterwarnings('ignore')\n\nwith open('../openai_api_key.txt', 'r') as f:\n    api_key = f.read()\n\nos.environ['OPENAI_API_KEY'] = api_key\n\nllm = OpenAI()\nchat = ChatOpenAI()\n\n\nset_llm_cache(InMemoryCache())\n</code></pre>"},{"location":"model_io/#steps-to-use-the-output-parser","title":"Steps to use the output parser","text":"<ul> <li>format_instructions</li> <li>parse</li> </ul> <p>Step 1: Create and instance of the parser</p> <pre><code>from langchain.output_parsers import CommaSeparatedListOutputParser\n\noutput_parser = CommaSeparatedListOutputParser()\n\noutput_parser\n</code></pre> <pre><code>&gt; CommaSeparatedListOutputParser()\n</code></pre> <p>Step 2: Get the format instructions</p> <pre><code>output_parser.get_format_instructions()\n</code></pre> <pre><code>&gt; 'Your response should be a list of comma separated values, eg: `foo, bar, baz`'\n</code></pre> <p>Step 3: Send the instructions to the model</p> <pre><code>from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n\nhuman_template = \"{user_request}\\n{format_instructions}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\nprompt = chat_prompt.format_prompt(user_request=\"What are the 7 wonders?\", format_instructions=output_parser.get_format_instructions())\n\nprompt\n</code></pre> <pre><code>&gt; ChatPromptValue(messages=[HumanMessage(content='What are the 7 wonders?\\nYour response should be a list of comma separated values, eg: `foo, bar, baz`')])\n</code></pre> <pre><code>messages = prompt.to_messages()\n\nresponse = chat(messages=messages)\n\nprint(response.content)\n</code></pre> <pre><code>&gt; Great Pyramid of Giza, Hanging Gardens of Babylon, Statue of Zeus at Olympia, Temple of Artemis at Ephesus, Mausoleum at Halicarnassus, Colossus of Rhodes, Lighthouse of Alexandria\n</code></pre> <p>Step 4: use the parser to parse the output</p> <pre><code>output_parser.parse(response.content)\n</code></pre> <pre><code>&gt; ['Great Pyramid of Giza',\n 'Hanging Gardens of Babylon',\n 'Statue of Zeus at Olympia',\n 'Temple of Artemis at Ephesus',\n 'Mausoleum at Halicarnassus',\n 'Colossus of Rhodes',\n 'Lighthouse of Alexandria']\n</code></pre>"},{"location":"model_io/#when-parser-fails","title":"When parser fails?","text":"<pre><code>from langchain.output_parsers import DatetimeOutputParser\n\noutput_parser = DatetimeOutputParser()\n\nformat_instructions = output_parser.get_format_instructions()\n\nprint(format_instructions)\n</code></pre> <pre><code>&gt; Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 0278-08-03T19:42:55.481110Z, 1567-04-05T01:30:42.197571Z, 0101-06-24T18:20:21.443663Z\n\nReturn ONLY this string, no other words!\n</code></pre> <pre><code>human_template = \"{human_messsage}\\n{format_instructions}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\nprompt = chat_prompt.format_prompt(human_messsage=\"When was Jesus Christ born?\", format_instructions=format_instructions)\nmessages = prompt.to_messages()\n\nresponse = chat(messages=messages)\n\noutput = output_parser.parse(response.content)\n\noutput\n</code></pre> <pre><code>&gt; ---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nFile d:\\CodeWork\\GitHub\\langchain_training\\.venv\\lib\\site-packages\\langchain\\output_parsers\\datetime.py:50, in DatetimeOutputParser.parse(self, response)\n     49 try:\n---&gt; 50     return datetime.strptime(response.strip(), self.format)\n     51 except ValueError as e: ...\n</code></pre>"},{"location":"model_io/#outputfixingparser","title":"OutputFixingParser","text":"<pre><code>from langchain.output_parsers import OutputFixingParser\n\nfixing_parser = OutputFixingParser.from_llm(parser=output_parser, llm=chat)\n\nfixed_output = fixing_parser.parse(response.content)\n\nfixed_output\n</code></pre> <pre><code>&gt; datetime.datetime(1, 1, 1, 0, 0)\n</code></pre> <p>Fixing might not always work, So let's try multiple times</p> <pre><code>for chance in range(1, 10):\n    try:\n        fixed_output = fixing_parser.parse(response.content)\n    except:\n        continue\n    else:\n        break\n\nfixed_output\n</code></pre> <pre><code>&gt; datetime.datetime(1, 1, 1, 0, 0)\n</code></pre>"},{"location":"model_io/#custom-parsers","title":"Custom Parsers","text":""},{"location":"model_io/#structured-output-parser","title":"Structured Output Parser","text":"<p>Define the response schema</p> <pre><code>from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n\nresponse_schemas = [\n    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n    ResponseSchema(\n        name=\"source\",\n        description=\"source used to answer the user's question, should be a website.\",\n    ),\n]\n</code></pre> <p>Define the output parser</p> <pre><code>from langchain.output_parsers import StructuredOutputParser\n\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\noutput_parser\n</code></pre> <pre><code>&gt; StructuredOutputParser(response_schemas=[ResponseSchema(name='answer', description=\"answer to the user's question\", type='string'), ResponseSchema(name='source', description=\"source used to answer the user's question, should be a website.\", type='string')])\n</code></pre> <p>Get the format instructions</p> <pre><code>format_instructions = output_parser.get_format_instructions()\nformat_instructions\n</code></pre> <pre><code>&gt; 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"answer\": string  // answer to the user\\'s question\\n\\t\"source\": string  // source used to answer the user\\'s question, should be a website.\\n}\\n```\n</code></pre> <p>Get the response</p> <pre><code>human_template = \"{human_message}\\n{format_instructions}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\nprompt = chat_prompt.format_prompt(human_message = \"What's the world's largest man made structure?\", format_instructions=format_instructions)\nmessages = prompt.to_messages()\n\nresponse = chat(messages=messages)\n\noutput = output_parser.parse(response.content)\n\noutput\n</code></pre> <pre><code>&gt; {'answer': 'The Great Wall of China',\n 'source': 'https://www.history.com/topics/great-wall-of-china'}\n</code></pre> <p>Let's look at the more powerful way of creating custom parser</p>"},{"location":"model_io/#pydanticoutputparser","title":"PydanticOutputParser","text":"<p>Let's quickly learn about pydantic</p> <p>Conventional pythonic way of building classes</p> <pre><code>class Student:\n    def __init__(self, name: str):\n        self.name = name\n\njohn = Student(name='John')\njohn.name\n</code></pre> <pre><code>&gt; 'John'\n</code></pre> <p>Similarily</p> <pre><code>jane = Student(name=1) # Taking int even after defining the name to be str\njane.name\n</code></pre> <pre><code>&gt; 1\n</code></pre> <pre><code>type(jane.name) # Returning int too\n\n# Conventional approach doesn't have strict type validation\n</code></pre> <pre><code>&gt; int\n</code></pre> <p>Pydantic has simple syntax with strict type validation</p> <pre><code>from pydantic import BaseModel\n\nclass Student(BaseModel):\n    name: str\n\njane = Student(name=1) # THIS WILL THROW AN ERROR\n\njane = Student(name='jane')\njane.name\n</code></pre> <pre><code>&gt; 'jane'\n</code></pre> <p>Let's get back to langchain</p> <p>When we want our output to be in a specific class object format</p> <p>First let's define the class</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\nclass Car(BaseModel):\n    name: str = Field(description=\"Name of the car\")\n    model_number: str = Field(description=\"Model number of the car\")\n    features: List[str] = Field(description=\"List of features of the car\")\n</code></pre> <p>create an instance of our custom parser</p> <pre><code>from langchain.output_parsers import PydanticOutputParser\n\noutput_parser = PydanticOutputParser(pydantic_object=Car)\n\nprint(output_parser.get_format_instructions())\n</code></pre> <pre><code>&gt; The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema ...\n</code></pre> <p>Getting the response</p> <pre><code>human_template = \"{human_message}\\n{format_instructions}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\nprompt = chat_prompt.format_prompt(human_message='Tell me about the most expensive car in the world',\n                                   format_instructions=output_parser.get_format_instructions())\n\nresponse = chat(messages=prompt.to_messages())\noutput = output_parser.parse(response.content)\n\noutput\n</code></pre> <pre><code>&gt; Car(name='Bugatti La Voiture Noire', model_number='Divo', features=['1500 horsepower engine', '8.0-liter quad-turbocharged W16 engine', 'carbon fiber body', 'top speed of 261 mph'])\n</code></pre> <pre><code>type(output)\n</code></pre> <pre><code>&gt; __main__.Car\n</code></pre>"},{"location":"model_io/#pydanticstructuredoutputparser","title":"PydanticStructuredOutputParser","text":"<p>The new ChatOpenAI model from langchain_openai supports <code>with_structured_output</code> method, which can take the pydantic models built with pydantic_v1 from langchain_core</p> <pre><code>pip install langchain_openai\n</code></pre> <p>We will still be using <code>from langchain.chat_models import ChatOpenAI</code> for loading the chat model, to follow the standard structure of langchain (though it is depreciated)</p> <pre><code>import os\nfrom typing import List\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nwith open('../openai_api_key.txt') as f:\n    os.environ['OPENAI_API_KEY'] = f.read()\n\nclass Car(BaseModel):\n    name: str = Field(description=\"Name of the car\")\n    model_number: str = Field(description=\"Model number of the car\")\n    features: List[str] = Field(description=\"List of features of the car\")\n\nmodel = ChatOpenAI()\nmodel_with_structure = model.with_structured_output(Car)\nmodel_with_structure.invoke('Tell me about the most expensive car in the world')\n</code></pre> <pre><code>&gt; Car(name='Bugatti La Voiture Noire', model_number='1', features=['Luxurious design', 'Powerful engine', 'Top speed of 261 mph', 'Exclusive and limited edition'])\n</code></pre>"},{"location":"model_io/#project-ideas","title":"Project ideas","text":"<ul> <li>Real time text translation</li> <li>Text Summarization tool</li> <li>Q&amp;A System</li> <li>Travel Planner</li> <li>Tweet Responder</li> </ul>"},{"location":"model_io/#exercise_1","title":"Exercise","text":"<p>Create a Smart Chef bot that can give you recipes based on the available food items you have in your kitchen.</p> <p>Let's build a gradio app</p> <pre><code>import os\nfrom typing import List\nimport gradio as gr\nfrom pydantic import Field, BaseModel\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\nfrom langchain.output_parsers import PydanticOutputParser\n\n# Creating the instance of the chat model\n\nwith open('openai_api_key.txt', 'r') as f:\n    api_key = f.read()\n\nos.environ['OPENAI_API_KEY'] = api_key\n\nchat = ChatOpenAI()\n\n# Define the Pydantic Model\n\nclass SmartChef(BaseModel):\n    name: str = Field(description=\"Name fo the dish\")\n    ingredients: dict = Field(description=\"Python dictionary of ingredients and their corresponding quantities as keys and values of the python dictionary respectively\")\n    instructions: List[str] = Field(description=\"Python list of instructions to prepare the dish\")\n\n# Get format instructions\n\nfrom langchain.output_parsers import PydanticOutputParser\n\noutput_parser = PydanticOutputParser(pydantic_object=SmartChef)\nformat_instructions = output_parser.get_format_instructions()\nformat_instructions\n\ndef smart_chef(food_items: str) -&gt; list:\n\n    # Getting the response\n    human_template = \"\"\"I have the following list of the food items:\n\n    {food_items}\n\n    Suggest me a recipe only using these food items\n\n    {format_instructions}\"\"\"\n\n    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n    prompt = chat_prompt.format_prompt(\n        food_items=food_items, format_instructions=format_instructions)\n\n    messages = prompt.to_messages()\n    response = chat(messages=messages)\n    output = output_parser.parse(response.content)\n\n    dish_name, ingredients, instructions = output.name, output.ingredients, output.instructions\n    return dish_name, ingredients, instructions\n\n# Building interface\nwith gr.Blocks() as demo:\n    gr.HTML(\"&lt;h1 align='center'&gt;Smart Chef&lt;/h1&gt;\")\n    gr.HTML(\"&lt;h3 align='center'&gt;&lt;i&gt;Cook with whatever you have&lt;/i&gt;&lt;/h3&gt;\")\n    inputs = [gr.Textbox(label='Enter the list of ingredients you have, in a comma separated text', lines=3, placeholder='Example: Chicken, Onion, Tomatoes, ... etc.')]\n    generate_btn = gr.Button(value=\"Generate\")\n    outputs = [gr.Text(label='Name of the dish'), gr.JSON(label=\"Ingredients with corresponding quantities\"), gr.Textbox(label=\"Instructions to prepare\")]\n    generate_btn.click(fn=smart_chef, inputs=inputs, outputs=outputs)\n\nif __name__==\"__main__\":\n    demo.launch(share=True)\n</code></pre> <p>In the terminal, run the following command</p> <pre><code>python src/app.py\n</code></pre> <p>Deploying Gradio application in HuggingFace Spaces * Create a HuggingFace account * Install Gitbash (Optional)</p> <p>For Deploying</p> <pre><code>import os\nfrom typing import List\nimport gradio as gr\n# from pydantic import Field, BaseModel\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\nfrom langchain.output_parsers import PydanticOutputParser\n\n# Creating the instance of the chat model\n\n# with open('openai_api_key.txt', 'r') as f:\n#     api_key = f.read()\n\n# os.environ['OPENAI_API_KEY'] = api_key\n\nchat = ChatOpenAI()\n\n# Define the Pydantic Model\n\nclass SmartChef(BaseModel):\n    name: str = Field(description=\"Name fo the dish\")\n    ingredients: dict = Field(description=\"Python dictionary of ingredients and their corresponding quantities as keys and values of the python dictionary respectively\")\n    instructions: List[str] = Field(description=\"Python list of instructions to prepare the dish\")\n\n# Get format instructions\n\nfrom langchain.output_parsers import PydanticOutputParser\n\noutput_parser = PydanticOutputParser(pydantic_object=SmartChef)\nformat_instructions = output_parser.get_format_instructions()\nformat_instructions\n\ndef smart_chef(food_items: str) -&gt; list:\n    # Getting the response\n\n    human_template = \"\"\"I have the following list of the food items:\n    {food_items}\n    Suggest me a recipe only using these food items\n    {format_instructions}\"\"\"\n\n    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n    prompt = chat_prompt.format_prompt(\n        food_items=food_items, format_instructions=format_instructions)\n\n    messages = prompt.to_messages()\n    response = chat(messages=messages)\n    output = output_parser.parse(response.content)\n\n    dish_name, ingredients, instructions = output.name, output.ingredients, output.instructions\n    return dish_name, ingredients, instructions\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\"&lt;h1 align='center'&gt;Smart Chef&lt;/h1&gt;\")\n    gr.HTML(\"&lt;h3 align='center'&gt;&lt;i&gt;Cook with whatever you have&lt;/i&gt;&lt;/h3&gt;\")\n    # gr.HTML(\"## Cook with whatever you have\")\n    inputs = [gr.Textbox(label='Enter the list of ingredients you have, in a comma separated text', lines=3, placeholder='Example: Chicken, Onion, Tomatoes, ... etc.')]\n    generate_btn = gr.Button(value=\"Generate\")\n    outputs = [gr.Text(label='Name of the dish'), gr.JSON(label=\"Ingredients with corresponding quantities\"), gr.Textbox(label=\"Instructions to prepare\")]\n    generate_btn.click(fn=smart_chef, inputs=inputs, outputs=outputs)\n\nif __name__==\"__main__\":\n    demo.launch()\n</code></pre>"},{"location":"retrieval/","title":"Retrieval","text":"<p>Large language models are trained on massive datasets, but they don't know everything. That's where Retrieval Augmented Generation (RAG) comes in, and LangChain has you covered with all the basics to Advanced bulding blocks:</p> <p></p> <ol> <li> <p>Document loaders:    Learn how to put your own information into the training mix.</p> </li> <li> <p>Text Splitters:    Find out how to tweak your data to make the model understand better.</p> </li> <li> <p>Text Embedding Models:    Discover ways to seamlessly include your information within the model.</p> </li> <li> <p>Vector Stores:    Explore ways to save these tweaks for quick and efficient retrieval.</p> </li> <li> <p>Retrievers:    Learn how to ask the model for the information you stored.</p> </li> </ol> <p>LangChain breaks down each step, making it easy for you to make the most of RAG with your language models.</p>"},{"location":"retrieval/#document-loaders","title":"Document Loaders","text":""},{"location":"retrieval/#csv-loader","title":"CSV Loader","text":"<pre><code>from langchain.document_loaders import CSVLoader\nloader = CSVLoader(file_path='../datasets/sns_datasets/titanic.csv') # Lazy Loader\nloader\n</code></pre> <pre><code>  &gt; &lt;langchain_community.document_loaders.csv_loader.CSVLoader at 0x147b62e5760&gt;\n</code></pre> <pre><code>data = loader.load()\n\ndata\n</code></pre> <pre><code>  &gt; [Document(page_content='survived: 0\\npclass: 3\\nsex: male\\nage: 22.0\\nsibsp: 1\\nparch: 0\\nfare: 7.25\\nembarked: S\\nclass: Third\\nwho: man\\nadult_male: True\\ndeck: \\nembark_town: Southampton\\nalive: no\\nalone: False', metadata={'source': '../datasets/sns_datasets/titanic.csv', 'row': 0}), Document(page_content='survived: 1\\npclass: 1\\nsex: female\\nage: 38.0\\nsibsp: 1\\nparch: 0\\nfare: 71.2833\\nembarked: C\\nclass: First\\nwho: woman\\nadult_male: False\\ndeck: C\\nembark_town: Cherbourg\\nalive: yes\\nalone: False', metadata={'source': '../datasets/sns_datasets/titanic.csv', 'row': 1}), ...]\n</code></pre> <p>Python list of document objects; Each row in a separate document object</p> <pre><code>data[0] # Single Row\n</code></pre> <pre><code>  &gt; Document(page_content='survived: 0\\npclass: 3\\nsex: male\\nage: 22.0\\nsibsp: 1\\nparch: 0\\nfare: 7.25\\nembarked: S\\nclass: Third\\nwho: man\\nadult_male: True\\ndeck: \\nembark_town: Southampton\\nalive: no\\nalone: False', metadata={'source': '../datasets/sns_datasets/titanic.csv', 'row': 0})\n</code></pre> <pre><code>type(data[0])\n</code></pre> <pre><code>  &gt; langchain_core.documents.base.Document\n</code></pre> <p>To get the document content</p> <pre><code>print(data[0].page_content)\n</code></pre> <pre><code>  &gt;  survived: 0\n     pclass: 3\n     sex: male\n     age: 22.0\n     sibsp: 1\n     parch: 0\n     fare: 7.25\n     embarked: S\n     class: Third\n     who: man\n     adult_male: True\n     deck: \n     embark_town: Southampton\n     alive: no\n     alone: False\n</code></pre> <p>To get the metadata</p> <pre><code>print(data[0].metadata)\n</code></pre> <pre><code>  &gt; {'source': '../datasets/sns_datasets/titanic.csv', 'row': 0}\n</code></pre> <p>Specify a column name to identify the dataset</p> <pre><code>data = CSVLoader(file_path='../datasets/sns_datasets/titanic.csv', source_column= 'sex').load()\n</code></pre>"},{"location":"retrieval/#html-loader","title":"HTML Loader","text":"<p>Similar syntax</p> <pre><code>from langchain.document_loaders import UnstructuredHTMLLoader\nloader = UnstructuredHTMLLoader('../datasets/harry_potter_html/001.htm')\ndata = loader.load()\ndata\n</code></pre> <pre><code>  &gt; [Document(page_content='A Day of Very Low Probability\\n\\nBeneath the moonlight glints a tiny fragment of silver, a fraction of a line\u2026\\n\\n ...\n</code></pre> <p>Loading HTML documents with BeautifulSoup</p> <pre><code>from langchain.document_loaders import BSHTMLLoader\nloader = BSHTMLLoader('../datasets/harry_potter_html/001.htm')\ndata = loader.load()\ndata\n</code></pre> <pre><code>  &gt; A Day of Very Low Probability\n\n  Beneath the moonlight glints a tiny fragment of silver, a fraction of a line\u2026\n  (black robes, falling)\n  \u2026blood spills out in litres, and someone screams a word. ...\n</code></pre> <p>This response is close to the content in the HTML file.</p>"},{"location":"retrieval/#markdown-loader","title":"Markdown Loader","text":"<pre><code>from langchain.document_loaders import UnstructuredMarkdownLoader\n\nmd_filepath = \"../datasets/harry_potter_md/001.md\"\n\nloader = UnstructuredMarkdownLoader(file_path=md_filepath)\n\ndata = loader.load()\n\ndata\n</code></pre> <pre><code>  &gt; [Document(page_content='A Day of Very Low Probability\\n\\nBeneath the moonlight glints a tiny fragment of silver ...\n</code></pre>"},{"location":"retrieval/#pdf-loader","title":"PDF Loader","text":"<pre><code>from langchain.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader('../datasets/harry_potter_pdf/hpmor-trade-classic.pdf')\n\ndata = loader.load()\n\ndata\n</code></pre> <pre><code>  [Document(page_content='Harry Potter and the Methods of Rationality', metadata={'source': '../datasets/harry_potter_pdf/hpmor-trade-classic.pdf', 'page': 0}),\n  Document(page_content='', metadata={'source': '../datasets/harry_potter_pdf/hpmor-trade-classic.pdf', 'page': 1}), ...\n</code></pre>"},{"location":"retrieval/#wikipedia","title":"Wikipedia","text":"<pre><code>from langchain.document_loaders import WikipediaLoader\nloader = WikipediaLoader(query='India', load_max_docs=1)\ndata = loader.load()\n\ndata\n</code></pre> <pre><code>  &gt; [Document(page_content=\"India, officially the Republic of India (ISO: Bh\u0101rat Ga\u1e47ar\u0101jya), ...\n</code></pre> <p>Since we are only loading one document, the number of document objects in the response list is also 1</p> <pre><code>len(data)\n</code></pre> <pre><code>  &gt; 1\n</code></pre> <p>To get the metadata</p> <pre><code># select the document object\ndata[0].metadata\n</code></pre> <pre><code>  &gt; {'title': 'India',\n  'summary': \"India, officially the Republic of India (ISO: Bh\u0101rat Ga\u1e47ar\u0101jya), ...\",\n  'source': 'https://en.wikipedia.org/wiki/India'}\n</code></pre> <p>Call the keys of the dictionary to get the specific information from the metadata.</p> <p>Information from the metadata can be used to filter you data in the later stages.</p>"},{"location":"retrieval/#arxiv-loader","title":"ArXiv Loader","text":"<p>Loading the content from the famous scientific article publisher</p> <p>To get the article IDs of any ArXiv papers, check the URL of the page or header of the page.</p> <p></p> <pre><code>from langchain_community.document_loaders import ArxivLoader\n\nloader = ArxivLoader(query='2201.03916', load_max_docs=1) # AutoRL paper (article ID -&gt; 2201.03916)\n\ndata = loader.load()\n\ndata\n</code></pre> <pre><code>  &gt; [Document(page_content='Journal of Arti\ufb01cial Intelligence Research 74 (2022) ...\n</code></pre> <pre><code>len(data) # since load_max_docs=1\n</code></pre> <pre><code>  &gt; 1\n</code></pre> <pre><code>print(data[0].page_content)\n</code></pre> <pre><code>  &gt; Journal of Arti\ufb01cial Intelligence Research 74 (2022) 517-568\n  Submitted 01/2022; published 06/2022\n  Automated Reinforcement Learning (AutoRL):\n</code></pre> <p>Getting the metadata similar to the previous steps</p> <pre><code>data[0].metadata\n</code></pre> <pre><code>  &gt; {'Published': '2022-06-02',\n  'Title': 'Automated Reinforcement Learning (AutoRL): A Survey and Open Problems',\n  'Authors': 'Jack Parker-Holder, Raghu Rajan, Xingyou Song, Andr\u00e9 Biedenkapp, Yingjie Miao, Theresa Eimer, Baohe Zhang, Vu Nguyen, Roberto Calandra, Aleksandra Faust, Frank Hutter, Marius Lindauer',\n  'Summary': 'The combination of Reinforcement Learning (RL) ...\"}\n</code></pre> <p>Let's connect the retrieved information to the LLM</p> <pre><code>import os\nfrom langchain_openai import ChatOpenAI\nfrom langchain.globals import set_llm_cache\nfrom langchain.cache import InMemoryCache\n\nwith open(\"../openai_api_key.txt\", 'r') as f:\n    os.environ['OPENAI_API_KEY'] = f.read()\n\nchat = ChatOpenAI()\nset_llm_cache(InMemoryCache())\n</code></pre> <pre><code># Setting up the prompt templates\n\nfrom langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n\nsystem_template = \"You are Peer Reviewer\"\nhuman_template = \"Read the paper with the title: '{title}'\\n\\nAnd Content: {content} and critically list down all the issues in the paper\"\n\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\nchat_prompt = ChatPromptTemplate.from_messages(messages=[system_message_prompt, human_message_prompt])\nprompt = chat_prompt.format_prompt(title=data[0].metadata['Title'], content=data[0].page_content)\nmessages = prompt.to_messages()\n\nresponse = chat(messages=messages)\n\nprint(response.content)\n</code></pre> <pre><code>  &gt; Overall, the paper \"Attention Is All You Need\" presents a novel model architecture, the Transformer, which is based solely on attention mechanisms and dispenses with recurrence and convolutions. The paper provides a detailed description of the model architecture, background information, model variations, training process, and results in machine translation and English constituency parsing tasks. The paper also includes attention visualizations to illustrate the behavior of the attention heads.\n\n  Here are some key points to consider for a critical review of the paper: ...\n</code></pre> <pre><code>def peer_review(article_id):\n    chat = ChatOpenAI(max_tokens=500)\n    loader = ArxivLoader(query=article_id, load_max_docs=1)\n    data = loader.load()\n    page_content = data[0].page_content\n    title = data[0].metadata['Title']\n    summary = data[0].metadata['Summary']\n\n    system_template = \"You are Peer Reviewer\"\n    human_template = \"Read the paper with the title: '{title}'\\n\\nAnd Content: {content} and critically list down all the issues in the paper\"\n\n    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\n    chat_prompt = ChatPromptTemplate.from_messages(messages=[system_message_prompt, human_message_prompt])\n\n    try:\n        prompt = chat_prompt.format_prompt(title=title, content=page_content) # Suggest not to go with this\n        messages = prompt.to_messages()\n        response = chat(messages=messages)\n    except:\n        prompt = chat_prompt.format_prompt(title=title, content=summary)\n        messages = prompt.to_messages()\n        response = chat(messages=messages)\n\n    return response.content\n</code></pre> <pre><code>print(peer_review(article_id='2201.03514')) # Black-Box Tuning for Language-Model-as-a-Service\n</code></pre> <pre><code>  &gt; After reviewing the paper titled 'Black-Box Tuning for Language-Model-as-a-Service', I have identified several issues in the paper that need to be addressed before it can be considered for publication:\n\n  1. Lack of Clarity in Problem Statement: The paper does not clearly define the problem statement or research question it aims to address. It is unclear why optimizing task prompts through black-box tuning for Language-Model-as-a-Service (LMaaS) is important or how it contributes to the existing body of knowledge.\n</code></pre>"},{"location":"retrieval/#text-splitter","title":"Text Splitter","text":""},{"location":"retrieval/#split-by-character","title":"Split by character","text":"<p>Reading the data</p> <pre><code>filepath = \"../datasets/Harry Potter 1 - Sorcerer's Stone.txt\"\n\nwith open(filepath, 'r') as f:\n    hp_book = f.read()\n\nprint(\"Number of characters letters in the document:\", len(hp_book))\nprint(\"Number of words in the document:\", len(hp_book.split()))\nprint(\"Number of lines in the document:\", len(hp_book.split(\"\\n\")))\n</code></pre> <pre><code>  Number of characters letters in the document: 439742\n  Number of words in the document: 78451\n  Number of lines in the document: 10703\n</code></pre> <p>To understand the how the number of characters if we use any separator manually</p> <pre><code>from collections import Counter\n\nline_len_list = []\n\nfor line in hp_book.split(\"\\n\"):\n    curr_line_len = len(line)\n    line_len_list.append(curr_line_len)\n\nCounter(line_len_list) # It show how many those chunks with the same character length is present\n</code></pre> <pre><code>  &gt; Counter({37: 57,\n        0: 3057,\n        11: 38,\n  ...\n        4: 15,\n        3: 9,\n        2: 1})\n</code></pre>"},{"location":"retrieval/#character-text-splitter","title":"Character Text Splitter","text":"<p>Splitting the text at a specific character only if the chunk exceeds the given chunk size</p> <pre><code>from langchain.text_splitter import CharacterTextSplitter\n\ndef len_func(text): # In this case, you can just use &gt;len&lt;\n    return len(text)\n\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\\n\",\n    chunk_size=1200,\n    chunk_overlap=100,\n    length_function=len_func,\n    is_separator_regex=False\n)\n\npara_list = text_splitter.create_documents(texts=[hp_book])\n\npara_list\n</code></pre> <pre><code>  &gt; [Document(page_content=\"Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\n  ...\n  I\\'m going to have a lot of fun with Dudley this summer....\"\\n\\nTHE END')]\n</code></pre> <p>To add metadata for the document objects</p> <pre><code>first_chunk = para_list[0]\n\n# Just assign/reassign\nfirst_chunk.metadata = {\"source\": filepath}\n\nfirst_chunk.metadata\n</code></pre> <pre><code>  &gt; {'source': \"../datasets/Harry Potter 1 - Sorcerer's Stone.txt\"}\n</code></pre> <p>What if the text exceeds the chunk length and there is not separator to chunk the text?</p> <pre><code># Adding the extra line\nextra_line = \" \".join(['word']*500)\n\npara_list = text_splitter.create_documents(texts = [extra_line + hp_book])\n\n# checking the length of the first line as the extra line is added there\nfirst_chunk_text = para_list[0].page_content\n\nlen(first_chunk_text)\n</code></pre> <pre><code>  Created a chunk of size 2536, which is longer than the specified 1200\n  &gt; 2536\n</code></pre> <p>Can we add multiple separators to make it working better?</p> <p>That's where Recursive Character Text Splitter comes in.</p>"},{"location":"retrieval/#recursive-character-splitter","title":"Recursive Character Splitter","text":"<p>It tries to split on them in order until the chunks are small enough. The default list is <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.</p> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\"\\n\\n\", \"\\n\", ' '],\n    chunk_size = 200,\n    chunk_overlap = 100,\n    length_function = len_func,\n    is_separator_regex=False\n)\n\n# Here, the split first happens at \"\\n\\n\", if the chunk size exceeds, it will move to the next separator, if it still exceeds, it will move to the next separator which is a \" \".\n\nchunk_list = text_splitter.create_documents(texts = [hp_book])\n\nchunk_list\n</code></pre> <pre><code>  &gt; [Document(page_content='CHAPTER ONE\\n\\nTHE BOY WHO LIVED'),\n  ...]\n</code></pre> <p>Let's see how this chunking process work in the previous scenario</p> <pre><code>chunk_list = text_splitter.create_documents(texts = [extra_line + hp_book]) # Adding the extra line\n\nchunk_list\n</code></pre> <pre><code>  &gt; [Document(page_content='word word word word word word ...\n\n  ...]\n</code></pre> <p>The text got chunked at spaces to maintain the chunk size in the first line.</p>"},{"location":"retrieval/#split-by-tokens","title":"Split by tokens","text":"<p>tiktoken is a python library developed by openAI to count the number of tokens in a string without making an API call.</p> <pre><code>pip install tiktoken\n</code></pre> <p>Splitting based on the token limit</p> <pre><code>from langchain.text_splitter import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    separator=\"\\n\\n\", \n    chunk_size=1200, \n    chunk_overlap=100, \n    is_separator_regex=False,\n    model_name='text-embedding-3-small',\n    encoding_name='text-embedding-3-small', # same as model name\n)\n\ndoc_list = text_splitter.create_documents([hp_book])\n\ndoc_list\n</code></pre> <pre><code>  &gt; [Document(page_content=\"Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\n  ...\n  I\\'m going to have a lot of fun with Dudley this summer....\"\\n\\nTHE END')]\n</code></pre> <p>The model name here refers to the model used for calculating the tokens.</p> <p>To split the text and return the text chunks</p> <pre><code>line_list = text_splitter.split_text(hp_book)\n\nline_list\n</code></pre> <pre><code>  &gt; ['Harry Potter and the Sorcerer\\'s Stone\\n\\n\\nCHAPTER ONE\\...\n  ...Dudley this summer....\"\\n\\nTHE END']\n</code></pre> <p>If you want to convert the split text into list of document objects</p> <pre><code>from langchain.docstore.document import Document\n\ndoc_list = []\n\nfor line in line_list:\n    curr_doc = Document(page_content=line, metadata={\"source\": filepath})\n    doc_list.append(curr_doc)\n\ndoc_list\n</code></pre> <pre><code>  &gt; [Document(page_content=\"Harry Potter and the Sorcerer's Stone\\n\\n\\nCHAPTER ONE\\n\\nTHE BOY WHO LIVED\n  ...\n  I\\'m going to have a lot of fun with Dudley this summer....\"\\n\\nTHE END')]\n</code></pre>"},{"location":"retrieval/#code-splitting","title":"Code Splitting","text":"<p>Let's learn a generic way of splitting code that's written in any language. For this let's convert the previous peer_review function code into text.</p> <pre><code>python_code = \"\"\"def peer_review(article_id):\n    chat = ChatOpenAI()\n    loader = ArxivLoader(query=article_id, load_max_docs=2)\n    data = loader.load()\n    first_record = data[0]\n    page_content = first_record.page_content\n    title = first_record.metadata['Title']\n    summary = first_record.metadata['Summary']\n\n    summary_list = []\n    for record in data:\n        summary_list.append(record.metadata['Summary'])\n    full_summary = \"\\n\\n\".join(summary_list)\n\n    system_template = \"You are a Peer Reviewer\"\n    human_template = \"Read the paper with the title: '{title}'\\n\\nAnd Content: {content} and critically list down all the issues in the paper\"\n\n    systemp_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\n    chat_prompt = ChatPromptTemplate.from_messages([systemp_message_prompt, human_message_prompt])\n    prompt = chat_prompt.format_prompt(title=title, content=page_content)\n\n    response = chat(messages = prompt.to_messages())\n\n    return response.content\"\"\"\n</code></pre> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n\ntext_splitter = RecursiveCharacterTextSplitter.from_language(\n    language=Language.PYTHON,\n    chunk_size=50,\n    chunk_overlap=10\n)\n\ntext_splitter.create_documents(texts = [python_code])\n</code></pre> <pre><code>  &gt; [Document(page_content='def peer_review(article_id):'),\n  Document(page_content='chat = ChatOpenAI()'),\n  ...\n  Document(page_content='= prompt.to_messages())'),\n  Document(page_content='return response.content')]\n</code></pre> <p>Similar to python code, you can also split any the code in programming language. For example: To split javascript code use <code>Language.JS</code></p>"},{"location":"retrieval/#semantic-chunking","title":"Semantic Chunking","text":""},{"location":"retrieval/#embeddings","title":"Embeddings","text":"<p>Embeddngs are stored along with their corresponding text in the vector database. When queried, the query text is converted to embeddngs using the same embedding function and find the similar embeddings in the vector database and returns corresponding matching text.</p>"},{"location":"retrieval/#openai-embeddings","title":"OpenAI Embeddings","text":"<pre><code>import os\nfrom langchain.embeddings import OpenAIEmbeddings\n\nwith open('../openai_api_key.txt', 'r') as f:\n    api_key = f.read()\n\nos.environ['OPENAI_API_KEY'] = api_key\n</code></pre> <p>Either set the environment variable or pass it as a keyword parameter as shown below, just like any llm</p> <pre><code>embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n</code></pre> <p>Creating a sample text</p> <pre><code>text = \"The scar had not pained Harry for nineteen years. All was well.\"\n</code></pre> <p>Embedding the text</p> <pre><code>embedded_text = embeddings.embed_query(text)\n\nembedded_text\n</code></pre> <pre><code>  &gt; [-0.00598582291957263,\n  0.02148159007746089,\n  ...]\n</code></pre> <pre><code>len(embedded_text)\n</code></pre> <pre><code>  &gt; 1536\n</code></pre> <pre><code># If we have n lines in langchain document format\nfrom langchain.docstore.document import Document\n\ndoc_lines = [\n    Document(page_content=text, metadata={\"source\": \"Harry Potter\"}),\n    Document(page_content=\"It is our choices, Harry, that show what we truly are, far more than our abilities\",\n             metadata={\"source\": \"Harry Potter\"})\n]\n\ndoc_lines # consider this to be the response from text splitters\n</code></pre> <pre><code>  &gt; [Document(page_content='The scar had not pained Harry for nineteen years. All was well.', metadata={'source': 'Harry Potter'}),\n  Document(page_content='It is our choices, Harry, that show what we truly are, far more than our abilities', metadata={'source': 'Harry Potter'})]\n</code></pre> <pre><code># We will extract the page content\n\nline_list = [doc.page_content for doc in doc_lines]\n</code></pre> <pre><code>embedded_docs = [embeddings.embed_query(line) for line in line_list]\n\nnp.array(embedded_docs).shape\n</code></pre> <pre><code>  &gt; (2, 1536)\n</code></pre> <ul> <li>OpenAI embeddings are not the best ranked MTEB (Massive text embedding benchmark) models (https://huggingface.co/spaces/mteb/leaderboard)</li> <li>On top of it, they are expensive.</li> <li>*So, let's explore some open source best performing text embedding models</li> </ul>"},{"location":"retrieval/#bge-embeddings","title":"BGE Embeddings","text":"<ul> <li>BGE models on the HuggingFace are the best open-source embedding models that are also integrated with langchain as of now.</li> <li>BGE Model is created by the Beijing Academy of Artificial Intelligence (BAAI).</li> <li>BAAI is a private non-profit organization engaged in AI research and development.</li> </ul> <pre><code>pip install sentence_transformers\n</code></pre> <pre><code>import numpy as np\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\n\nmodel_name = \"BAAI/bge-base-en-v1.5\"\nmodel_kwargs = {\"device\": \"cpu\"}\nencode_kwargs = {\"normalize_embeddings\": True}\n\nhf = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n</code></pre>"},{"location":"retrieval/#fake-embeddings","title":"Fake Embeddings","text":"<p>For testing purposes, if you have some hardware restrictions, you can use Fake Embeddings from Langchain</p> <pre><code>from langchain_community.embeddings import FakeEmbeddings\nfake_embeddings = FakeEmbeddings(size=300) # Define the embedding size\n\nfake_embedded_record = fake_embeddings.embed_query(\"This is some random text\")\nfake_embedded_records = fake_embeddings.embed_documents([\"This is some random text\"])\n</code></pre> <p>Single record</p> <pre><code>np.array(fake_embedded_record).shape\n</code></pre> <pre><code>  (300,)\n</code></pre> <p>For multiple records</p> <pre><code>np.array(fake_embedded_records).shape\n</code></pre> <pre><code>  (1, 300)\n</code></pre>"},{"location":"retrieval/#vector-store","title":"Vector Store","text":"<pre><code>pip install chromadb qdrant-client faiss-cpu\n</code></pre> <p>Imports</p> <pre><code>from langchain_community.document_loaders import WikipediaLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_community.vectorstores import FAISS\n</code></pre> <p>Document Loading</p> <pre><code>loader = WikipediaLoader(query='Elon Musk', load_max_docs=5)\ndocuments = loader.load()\ndocuments\n</code></pre> <pre><code>  &gt; [Document(page_content='Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman...)\n  Document(page_content='Business magnate Elon Musk initiated an acquisition of American...\n  Document(page_content='Elon Musk completed his acquisition of Twitter...\n  Document(page_content='The Musk family is a wealthy family of South African origin...\n  Document(page_content='Elon Musk is the CEO or owner of multiple companies including Tesla...]\n</code></pre> <p>Text splitting</p> <pre><code>text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\ndocs = text_splitter.split_documents(documents=documents)\nprint(len(docs))\ndocs\n</code></pre> <pre><code>  &gt; [Document(page_content='Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor...\n  Document(page_content='of Neuralink and OpenAI; and president of the Musk Foundation....\n  Document(page_content=\"Tesla and SpaceX.A member of the wealthy South African Musk family,...\n  ...]\n</code></pre> <p>Defining the embedding function</p> <pre><code>model_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cpu'}\nencode_kwargs = {\"normalize_embeddings\": True}\n\nembedding_function = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n</code></pre> <p>Query:</p> <pre><code>query = \"Who is elon musk's father?\"\n</code></pre>"},{"location":"retrieval/#faiss","title":"FAISS","text":"<p>Creating a vector database (FAISS - in memory database)</p> <pre><code>db = FAISS.from_documents(\n    docs,\n    embedding_function\n)\n</code></pre> <p>Querying the vector database</p> <pre><code>matched_docs = db.similarity_search(query=query, k=5)\n\nmatched_docs\n</code></pre> <pre><code>  &gt; [Document(page_content=\"Elon Musk's paternal great-grandmother was a Dutchwoman descended from the Dutch Free Burghers, while one of his maternal great-grandparents came from Switzerland. Elon Musk's father, Errol Musk, is a South African former electrical ...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'}),\n  Document(page_content=\"Elon Reeve Musk was born on June 28, 1971, in Pretoria, South Africa's administrative capital. ...,\n  Document(page_content='Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor....,\n  Document(page_content='father, Errol Musk, is a South African former electrical and mechanical engineer consultant and property developer, ...,\n  Document(page_content=\"Elon Musk (born 1971), entrepreneur and business magnate. Variously CEO, CTO, and/or Chairman of SpaceX, Tesla, Twitter (now X), and Neuralink...]\n</code></pre> <p>Check if the answer is present in results</p> <pre><code>['errol musk' in doc.page_content.lower() for doc in matched_docs] # Errol Musk is the answer\n</code></pre> <pre><code>  &gt; [True, True, False, True, False]\n</code></pre> <p>FAISS is an in-memory vector store.</p> <p>And most of the times, we don't use these in-memory vector stores.</p>"},{"location":"retrieval/#chromadb","title":"ChromaDB","text":"<p>Let's start with chromaDB and understand how to... * Save the vector store * Load the vector store * Add new records to vector store.</p> <pre><code>import chromadb\nfrom langchain.vectorstores import Chroma\n</code></pre> <p>Creating a Chroma Vector Store</p> <pre><code>db = Chroma.from_documents(docs, embedding_function, persist_directory='../output/elon_musk_db')\n</code></pre> <p>Loading the db</p> <pre><code>laoded_db = Chroma(persist_directory=\"../output/elon_musk_db\", embedding_function=embedding_function)\n</code></pre> <p>Querying the DBs</p> <pre><code>print(query)\n\nmatched_docs = db.similarity_search(\n    query = query,\n    k = 5\n)\n\nmatched_docs\n</code></pre> <pre><code>  Who is elon musk's father?\n  &gt; [Document(page_content=\"Elon Musk's paternal great-grandmother was a Dutchwoman descended from the Dutch Free Burghers, while one of his maternal great-grandparents came from Switzerland. Elon Musk's father, Errol Musk, is a South African former electrical ...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'}),\n  Document(page_content=\"Elon Reeve Musk was born on June 28, 1971, in Pretoria, South Africa's administrative capital. ...,\n  Document(page_content='Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor....,\n  Document(page_content='father, Errol Musk, is a South African former electrical and mechanical engineer consultant and property developer, ...,\n  Document(page_content=\"Elon Musk (born 1971), entrepreneur and business magnate. Variously CEO, CTO, and/or Chairman of SpaceX, Tesla, Twitter (now X), and Neuralink...]\n</code></pre> <p>As the embedding function and the text splitter are same in both the previous FAISS and current ChromaDB, the results are also the same.</p> <pre><code>['errol musk' in doc.page_content.lower() for doc in matched_docs]\n</code></pre> <pre><code>  &gt; [True, True, False, True, False]\n</code></pre> <p>Adding a new record to the existing vector store</p> <pre><code>family_data_loader = WikipediaLoader(query='Musk Family', load_max_docs=1)\nfamily_documents = family_data_loader.load()\nfamily_documents\n</code></pre> <pre><code>  &gt; [Document(page_content='The Musk family is a wealthy family of South African origin that is largely active in the United States and Canada. ... metadata={'title': 'Musk family', 'summary': 'The Musk family is a wealthy family ...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'})]\n</code></pre> <p>Using the exising text splitter</p> <pre><code>family_docs = text_splitter.split_documents(documents=family_documents)\nprint(len(family_docs))\nfamily_docs\n</code></pre> <p>11       [Document(page_content='The Musk family is a wealthy family of South African...', metadata={'title': 'Musk family', 'summary': 'The Musk family is a wealthy family...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'}),       Document(page_content='in the world, with an estimated net worth of US$232 billion ...', 'summary': 'The Musk family is a wealthy family...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'}),       Document(page_content='== History ==', metadata={'title': 'Musk family', 'summary': 'The Musk...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'}),       Document(page_content=\"Elon Musk's paternal great-grandmother...', metadata={'title': 'Musk family', 'summary': 'The Musk family ...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'}),       ...]</p> <p>Using the same loaded embedded function</p> <pre><code>db = Chroma.from_documents(\n    family_docs, # The new docs that we want to add\n    embedding_function, # Should be the same embedding function\n    persist_directory=\"../output/elon_musk_db\" # Existing vectorstore where we want to add the new records\n)\n</code></pre> <p>Getting the matching documents with the query</p> <pre><code>matched_docs = db.similarity_search(query=query, k=5)\n\n['errol musk' in doc.page_content.lower() for doc in matched_docs]\n</code></pre> <pre><code>  &gt; [True, True, True, False, True]\n</code></pre> <p>More number of records are getting matched.</p>"},{"location":"retrieval/#retrievers","title":"Retrievers","text":"<p>Making a retriever from vector store</p> <p>We can also define how the vectorstores should search and how many items to return</p>"},{"location":"retrieval/#vector-store-backed-retriever","title":"Vector store-backed retriever","text":"<pre><code>retriever = db.as_retriever()\n\nretriever\n</code></pre> <pre><code>  &gt; VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=&lt;langchain_community.vectorstores.chroma.Chroma object at 0x000001B97D293EE0&gt;\n</code></pre> <p>Querying a retriever</p> <pre><code>matched_docs = retriever.get_relevant_documents(query=query)\n\nmatched_docs\n</code></pre> <pre><code>&gt; [Document(page_content=\"Elon Musk's paternal great-grandmother was a Dutchwoman descended from the Dutch Free Burghers, while one of his maternal great-grandparents came from Switzerland. Elon Musk's father, Errol Musk, is a South African former electrical ...', 'source': 'https://en.wikipedia.org/wiki/Musk_family'})\"),\nDocument(page_content=\"Elon Reeve Musk was born on June 28, 1971, in Pretoria, South Africa's administrative capital. ...\"),\nDocument(page_content='Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor....'),\nDocument(page_content='father, Errol Musk, is a South African former electrical and mechanical engineer consultant and property developer, ...'),\nDocument(page_content=\"Elon Musk (born 1971), entrepreneur and business magnate. Variously CEO, CTO, and/or Chairman of SpaceX, Tesla, Twitter (now X), and Neuralink...]\")\n</code></pre> <pre><code>len(matched_docs)\n</code></pre> <pre><code>  &gt; 4\n</code></pre> <p>MMR - Maximum marginal relevance (relevancy and diversity)</p> <pre><code>retriever = db.as_retriever(search_type='mmr', search_kwargs={\"k\": 1})\n\nmatched_docs = retriever.get_relevant_documents(query=query)\n\nmatched_docs\n</code></pre> <pre><code>  &gt; [Document(page_content=\"Elon Musk's paternal great-grandmother was a Dutchwoman descended from the Dutch Free Burghers, ...\", metadata={'source': 'https://en.wikipedia.org/wiki/Musk_family', 'summary': 'The Musk family ...', 'title': 'Musk family'})]\n</code></pre> <p>Similarity Score threshold</p> <pre><code>retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5})\n\nmatched_docs = retriever.get_relevant_documents(query=query)\n\nmatched_docs\n</code></pre>"},{"location":"retrieval/#advanced-retrievers","title":"Advanced Retrievers","text":"<p>Let's setup the stage for the working on the following advanced retrievers</p> <pre><code># Imports\nimport chromadb\nfrom dotenv import load_dotenv\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain_community.document_loaders import WikipediaLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain.vectorstores import Chroma\n\nchunk_size = 400\nchunk_overlap = 100\n\n# Loading the environment variables\nload_dotenv()\n\n# Loading the chat model\nchat = ChatOpenAI()\n\n# Loading data\nloader = WikipediaLoader(query='Steve Jobs', load_max_docs=5)\ndocuments = loader.load()\n\n# Text splitting\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\ndocs = text_splitter.split_documents(documents=documents)\n\n# Embedding function\nembedding_function = HuggingFaceBgeEmbeddings(\n    model_name=\"BAAI/bge-large-en-v1.5\",\n    model_kwargs={'device': 'cpu'},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\n\n# Vector store\ndb = Chroma.from_documents(docs, embedding_function, persist_directory=\"../output/steve_jobs_db\")\n\n# Retriever\nretriever = db.as_retriever()\n\n# Query\nquery = \"When was Steve Jobs fired from Apple?\"\n</code></pre>"},{"location":"retrieval/#multi-query-retriever","title":"Multi-Query Retriever","text":"<p>For paraphrasing queries in different ways to reduce the emphasis on the way the query is written and increase on the emphasis on the key points of the query.</p> <p>LLMs are used to do this.</p> <pre><code>from langchain.retrievers.multi_query import MultiQueryRetriever\n\nretriever_from_llm = MultiQueryRetriever.from_llm(\n    retriever=retriever,\n    llm=chat\n)\n</code></pre> <p>To understand how to multiple queries are genered in the background; Let's define a logger.</p> <pre><code>import logging\nlogging.basicConfig()\nlogging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n</code></pre> <p>Get the responses from the vector database</p> <p>Since, we performed logging we can see the generated queries</p> <pre><code>retrieved_docs = retriever_from_llm.get_relevant_documents(query=query)\n\nretrieved_docs\n</code></pre> <pre><code>  INFO:langchain.retrievers.multi_query:Generated queries: [\"1. What was the date of Steve Jobs' departure from Apple due to termination?\", '2. At what point in time was Steve Jobs ousted from his position at Apple?', \"3. When did Steve Jobs face dismissal from Apple's leadership role?\"]\n</code></pre> <pre><code>&gt; [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board ...\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, ...', 'title': 'Steve Jobs'}),\n...\n Document(page_content=\"== Plot ==\\nIn 1984, the Apple Macintosh 128K's voice demo fails ...\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs_(film)', 'summary': \"Steve Jobs is a 2015 biographical drama film directed by Danny Boyle ...\", 'title': 'Steve Jobs (film)'})]\n</code></pre> <p>Note: Though we are using LLMs in our retreiver, it won't answer any of our questions. It will only return the top k records/chunks that can be helpful to answer our questions.</p>"},{"location":"retrieval/#contextual-compression","title":"Contextual Compression","text":"<p>Multiquery retriever works at the input side.</p> <p>With Contextual compression we will use the LLM to distill the extracted data from the retriever to give us the most relevant information for our query.</p> <p>Steps to follow</p> <ul> <li>Step 1: Connect to the database</li> <li>Step 1.1: Specify the user query</li> <li>Step 1.2: Get the most relevant documents for thre user quey.</li> <li>Step 2: Convert the db to retriever</li> <li>Step 3: Create an instance of LLM with temperature=0</li> <li>Step 4: Create an instance of the compressor</li> <li>Step 5: Create an instance of the compression_retriever</li> <li>Step 6: Get the most relevant documents for the given query</li> <li>Step 7: Explore the results</li> </ul> <pre><code># Connect to the database\ndb = Chroma(persist_directory=\"../output/steve_jobs_db\", embedding_function=embedding_function)\n\n# Specify the query\nquery = \"When was Steve Jobs fired from Apple?\"\n\n# Get the most relevant documents\nsim_docs = db.similarity_search(query=query)\n\nsim_docs\n</code></pre> <pre><code>&gt; [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley...\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology giant Apple Inc. ...', 'title': 'Steve Jobs'}),\n ...\nDocument(page_content='Jobs, specifically ahead of three press conferences he gave during that time ...', metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs_(film)', 'summary': \"Steve Jobs is a 2015 biographical drama film directed by Danny Boyle...\", 'title': 'Steve Jobs (film)'})]\n</code></pre> <p>Vector store-backed retriever</p> <pre><code>retriever = db.as_retriever()\nretriever\n</code></pre> <pre><code>  &gt; VectorStoreRetriever(tags=['Chroma', 'HuggingFaceBgeEmbeddings'], vectorstore=&lt;langchain_community.vectorstores.chroma.Chroma object at 0x0000026084293910&gt;)\n</code></pre> <p>Create an instance of llm/chat model with temperature 0</p> <pre><code>load_dotenv()\nchat = ChatOpenAI(temperature=0) # To get the same results all the time; as the response goes to the query later on.\n</code></pre> <p>Document Compressor</p> <pre><code>from langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(llm=chat)\n\ncompressor\n</code></pre> <pre><code>  &gt; LLMChainExtractor(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=...)\n</code></pre> <pre><code>print(compressor.llm_chain.prompt.template)\n</code></pre> <pre><code>  Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT.\n\n  Remember, *DO NOT* edit the extracted parts of the context.\n\n  &gt; Question: {question}\n  &gt; Context:\n  &gt;&gt;&gt;\n  {context}\n  &gt;&gt;&gt;\n  Extracted relevant parts:\n</code></pre> <p>Compression Retriever</p> <pre><code>from langchain.retrievers import ContextualCompressionRetriever\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor, base_retriever=retriever)\n\ncompression_retriever\n</code></pre> <pre><code>  &gt; ContextualCompressionRetriever(base_compressor=LLMChainExtractor(llm_chain=LLMChain(prompt=PromptTemplate(...)))\n</code></pre> <pre><code>matching_docs = compression_retriever.get_relevant_documents(query=query)\n\nmatching_docs\n</code></pre> <pre><code>  &gt; [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley.\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, ...', 'title': 'Steve Jobs'}),\n  Document(page_content='Jobs was actually forced out by the Apple board', metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs_(film)', 'summary': \"Steve Jobs is a 2015 biographical drama film directed by Danny Boyle and written by ...\", 'title': 'Steve Jobs (film)'})]\n</code></pre> <p>The results are the extract of the loaded data only and not the generated responses</p>"},{"location":"retrieval/#parent-document-retriever","title":"Parent Document Retriever","text":"<p>The embeddings of the smaller text chunks can reflect the meaning better than the larger chunks. But, you would also want larger chunks with more context for the LLMs to answer the given question.</p> <p>Parent Document Retriever helps to find the balance between both the cases. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.</p> <p>Let's understand how to build it using langchain.</p> <pre><code># Imports\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\n\n# Text splitters\nparent_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=1000, chunk_overlap=100)\nchild_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=200, chunk_overlap=50)\n\n# Temporary storage for parent documents\nstore = InMemoryStore()\n\n# Creating an instance of parent document retriever\npar_doc_retriever = ParentDocumentRetriever(\n    vectorstore=db,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Adding documents to the retriever\npar_doc_retriever.add_documents(docs)\n\n# Getting the parent content for the query\nmatched_par_docs = par_doc_retriever.get_relevant_documents(query=query)\n\nmatched_par_docs\n</code></pre> <pre><code>  &gt; [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, ...\", metadata={'title': 'Steve Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, ...', 'source': 'https://en.wikipedia.org/wiki/Steve_Jobs'}),\n  Document(page_content='Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, inventor...', metadata={'title': 'Steve Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman...', 'source': 'https://en.wikipedia.org/wiki/Steve_Jobs'})]\n</code></pre>"},{"location":"retrieval/#time-weighted-vector-store-retriever","title":"Time-weighted Vector Store Retriever","text":"<p>when used time-weighted vector store retriever the latest records have more weight along with the semantic similarity.</p> <p>matching_score = semantic_similarity + (1.0 - decay_rate) <sup>hours_passed</sup></p> <pre><code># Import\nimport faiss\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain.retrievers import TimeWeightedVectorStoreRetriever\n</code></pre> <p>As we are adding datetime in metadata, let's create temporary database</p> <pre><code>emb_size = 1024\nindex = faiss.IndexFlatL2(emb_size)\ntemp_db = FAISS(embedding_function, index,\n                docstore=InMemoryDocstore({}), index_to_docstore_id={})\n</code></pre> <p>Create an instance of the Time Weighted Vector Store Retriever</p> <pre><code>tw_retriever = TimeWeightedVectorStoreRetriever(\n    vectorstore=temp_db, decay_rate=0.8, k=1)\n</code></pre> <p>Let's add some documents with time stamps</p> <pre><code>from datetime import datetime, timedelta\nfrom langchain_core.documents import Document\n\n# yesterday = str(datetime.now() - timedelta(days=1))\nyesterday = datetime.now() - timedelta(days=1)\ntw_retriever.add_documents(\n    [Document(page_content=\"what is John doing?\", metadata={\"last_accessed_at\": yesterday})]\n)\n</code></pre> <pre><code>tw_retriever.add_documents([Document(page_content=\"what is Jack doing?\")])\ntw_retriever.get_relevant_documents(\"What are you doing?\")\n</code></pre> <pre><code>  &gt; [Document(page_content='what is Jack doing?', metadata={'last_accessed_at': datetime.datetime(2024, 4, 21, 9, 34, 58, 641604), 'created_at': datetime.datetime(2024, 4, 21, 9, 34, 58, 23364), 'buffer_idx': 1})]\n</code></pre> <p>For the above question, there should be an equal chance of returning both the added docuemnts. Since, the document with text \"What is Jack doing?\" is added later; It has been returned as TWVS Retriever considers time.</p>"},{"location":"retrieval/#hypothetical-document-retriever","title":"Hypothetical Document Retriever","text":"<p>One of the key issues in retrieving a document for a given query is, we are matching the query with the content that we believe has the information to answer the query. Essentially, we are comparing question with answer and retrieving based on the similarity score we got.</p> <p>To have a proper apples-to-apples comparison, it's good to match answers to answers rather than questions to answers.</p> <p>That's where, Hypothetical document embeddings come in handy.</p> <p>Other Retrivers:</p> <p>Query -&gt; Embeddings - Similarity Matching with the Embeddings in the vector store -&gt; Return the matched documents from vector store.</p> <p>Hypothetical Document Retriever:</p> <p>Query -&gt; LLM -&gt; Answer for the Query -&gt; Embeddings - Similarity Matching with the Embeddings in the vector store -&gt; Return the matched documents from vector store.</p>"},{"location":"retrieval/#hyde-from-scratch","title":"HyDE from Scratch","text":"<pre><code>from langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate\n\ndef get_hypo_doc(query):\n    template = \"\"\"Imagine you are an expert writing a detailed answer for the given query: '{query}'.\n    Your response should be comprehensive and include key points that would be found in a top search result.\"\"\"\n    system_message_prompt = SystemMessagePromptTemplate.from_template(\n        template=template)\n    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n    messages = chat_prompt.format_prompt(query=query).to_messages()\n    response = chat(messages=messages)\n    hypo_doc = response.content\n    return hypo_doc\n\nbase_retriever = db.as_retriever(search_kwargs={\"k\": 1})\n\nmatched_doc = base_retriever.get_relevant_documents(query=get_hypo_doc(query=query))\n\nmatched_doc\n</code></pre> <pre><code>  [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley...\", metadata={'doc_id': '0ee72998-1887-4e48-b316-2b7284fe9808', 'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman...', 'title': 'Steve Jobs'})]\n</code></pre>"},{"location":"retrieval/#hyde-from-chains","title":"HyDE from chains","text":"<p>Generate hypothetical documents to answer the query using the LLM, get their embeddings, and ask</p> <pre><code>from langchain.chains import HypotheticalDocumentEmbedder\n\nhyde_embedding_function = HypotheticalDocumentEmbedder.from_llm(llm=chat, base_embeddings=embedding_function, prompt_key=\"web_search\")\n</code></pre> <p>Default prompts: ['web_search', 'sci_fact', 'arguana', 'trec_covid', 'fiqa', 'dbpedia_entity', 'trec_news', 'mr_tydi']</p> <p>All these prompts are present in the paper: https://arxiv.org/pdf/2212.10496.pdf</p> <ul> <li><code>web_search</code>: This key is likely used for general web search tasks where the goal is to retrieve the most relevant documents from the web based on a user\u2019s query.</li> <li><code>sci_fact</code>: This could be related to scientific fact verification, where the system retrieves documents that can confirm or refute a scientific claim.</li> <li><code>arguana</code>: This key might be for argument understanding and analysis, possibly to retrieve documents that contain arguments or discussions on a given topic.</li> <li><code>trec_covid</code>: This is likely associated with the TREC-COVID challenge, which involves retrieving scientific literature related to COVID-19. fiqa: Stands for Financial Opinion Mining and Question Answering, which involves retrieving information relevant to financial domains.</li> <li><code>dbpedia_entity</code>: This key suggests a task related to retrieving information about entities from the structured data in DBpedia, which is a crowd-sourced community effort to extract structured content from the information created in various Wikimedia projects.</li> <li><code>trec_news</code>: This could be associated with the Text REtrieval Conference (TREC) News Track, focusing on news-related document retrieval.</li> <li><code>mr_tydi</code>: This key might refer to a multilingual retrieval task, possibly related to the Typologically Diverse Information Retrieval (TyDi) dataset, which focuses on information-seeking questions in multiple languages.</li> </ul> <p>Either assign one of these prompt_keys or create your own prompt_key. (Let's cover this after we start chains, if you hare interested.)</p> <p>Creating the database with Hypothetical Document Embedding function</p> <pre><code>doc_db = Chroma.from_documents(docs, hyde_embedding_function, persist_directory='../output/steve_job_hyde')\n</code></pre> <p>Gettting the matching documents</p> <pre><code>matched_docs = doc_db.similarity_search(query)\n\nmatched_docs\n</code></pre> <p>Note: Hypothetical Document Retriever works better than other retrievers in most the cases only when the LLM has some knowledge about the asked question. If the LLM has no clue of the asked question, the results can be quite messy. So, look at the responses of the LLM for the sample set of the questions before proceeding with it.</p>"},{"location":"retrieval/#ensemble-retriever","title":"Ensemble Retriever","text":"<p>Using Ensemble Retriever, we can combine multiple retrievers and get the relevant documents after querying based on all the results from all retrievers after reranking them using Reciprocal Rank Fusion Algorithm.</p> <p>Let's build a Hybrid Search Retriever using a sparse retriever like BM25 and Dense Retriever like parent document retriever.</p> <p>keyword search + semantic search</p> <pre><code># Import\nfrom langchain.retrievers import EnsembleRetriever\n\n# Creating an instance of the retriever using the previously defined retrievers in the above.\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, par_doc_retriever], weights=[0.5, 0.5]\n)\n\n# Getting the hybrid matched documents\nhybrid_matched_docs = ensemble_retriever.get_relevant_documents(query=query)\n\nhybrid_matched_docs\n</code></pre> <pre><code>  &gt; [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley...\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology giant Apple Inc. ...', 'title': 'Steve Jobs'}),\n  ...\n  Document(page_content='Jobs, specifically ahead of three press conferences he gave during that time ...', metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs_(film)', 'summary': \"Steve Jobs is a 2015 biographical drama film directed by Danny Boyle...\", 'title': 'Steve Jobs (film)'})]\n</code></pre>"},{"location":"retrieval/#filter-embedding-redundant-filter","title":"Filter - Embedding Redundant Filter","text":"<p>Filter that drops redundant documents by comparing their embeddings.</p> <pre><code>from langchain.document_transformers import EmbeddingsRedundantFilter\n\nredundant_filter = EmbeddingsRedundantFilter(embeddings=embedding_function) # Or hyde_embedding_function\n\nredundant_filter.transform_documents(hybrid_matched_docs)\n</code></pre> <pre><code>  &gt; [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley...\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology giant Apple Inc. ...', 'title': 'Steve Jobs'}),\n  ...\n  Document(page_content='Jobs, specifically ahead of three press conferences he gave during that time ...', metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs_(film)', 'summary': \"Steve Jobs is a 2015 biographical drama film directed by Danny Boyle...\", 'title': 'Steve Jobs (film)'})]\n</code></pre> <p>If there are any redundant documents, they will be dropped. In this case, all the 4 results are relevant. Thus, the filter is not dropping anything.</p>"},{"location":"retrieval/#filter-embedding-filter","title":"Filter - Embedding Filter","text":"<p>Document compressor that uses embeddings to drop documents unrelated to the query.</p> <pre><code>from langchain.retrievers.document_compressors import EmbeddingsFilter\n\nembdeddings_filter = EmbeddingsFilter(embeddings=embedding_function)\n\nembdeddings_filter.compress_documents(hybrid_matched_docs, query=query)\n</code></pre> <pre><code>  &gt; [Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley...\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology giant Apple Inc. ...', 'title': 'Steve Jobs'}),\n  ...\n  Document(page_content='Jobs, specifically ahead of three press conferences he gave during that time ...', metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs_(film)', 'summary': \"Steve Jobs is a 2015 biographical drama film directed by Danny Boyle...\", 'title': 'Steve Jobs (film)'})]\n</code></pre> <p>The documents that are far from the query embeddings in the embedding space will be dropped. In this case, all the 4 results are relevant. Thus, the filter is not dropping anything.</p>"},{"location":"retrieval/#reordering-long-content-reorder","title":"Reordering - Long Content Reorder","text":"<p>Important docs will be moved to beggining and the end</p> <pre><code>from langchain_community.document_transformers import LongContextReorder\n\nreorder = LongContextReorder()\n\nreordered_docs = reorder.transform_documents(hybrid_matched_docs)\n\nreordered_docs\n</code></pre> <pre><code>  &gt; [Document(page_content=\"Apple CEO John Sculley demands to know why the world believes he fired Jobs...', metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology giant Apple Inc. ...', 'title': 'Steve Jobs'}),\n  ...\n  Document(page_content=\"In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley...\", metadata={'source': 'https://en.wikipedia.org/wiki/Steve_Jobs', 'summary': 'Steven Paul Jobs (February 24, 1955 \u2013 October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology giant Apple Inc. ...', 'title': 'Steve Jobs'})]\n</code></pre>"},{"location":"retrieval/#rag-pipelines","title":"RAG Pipelines","text":"<p>Let's setup the stage for the working on the RAG Pipelines</p> <p>Loading the chat Model</p> <pre><code>from dotenv import load_dotenv\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.cache import InMemoryCache\nfrom langchain.globals import set_llm_cache\n\nload_dotenv()\n\nchat = ChatOpenAI()\nset_llm_cache(InMemoryCache())\n</code></pre> <p>Setting up the data loader</p> <pre><code>from langchain_community.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader(file_path='../datasets/udhr_booklet_en_web.pdf')\n\ndocuments = loader.load()\n\ndocuments\n</code></pre> <pre><code>  &gt; [Document(page_content='', metadata={'source': '../datasets/udhr_booklet_en_web.pdf', 'page': 0}),\n  Document(page_content='\u00a9 2015 United Nations  \\nAll rights reserved worldwide\\nIllustrations by Yacine Ait Kaci (YAK)\\nThis illustrated edition of the Universal Declaration of Human Rights \\n(UDHR)  is published by the United Nations...', metadata={'source': '../datasets/udhr_booklet_en_web.pdf', 'page': 1}),\n  ...\n  Document(page_content='| Universal Declaration of Human Rights | 62\\nArticleUNITED\\nNATIONS\\nNothing in this Declaration ...', metadata={'source': '../datasets/udhr_booklet_en_web.pdf', 'page': 70}),\n  Document(page_content='', metadata={'source': '../datasets/udhr_booklet_en_web.pdf', 'page': 71})]\n</code></pre> <p>Setting up the text splitter</p> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nchunk_size = 500\nchunk_overlap = 100\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\ndocs = text_splitter.split_documents(documents=documents)\n\nlen(docs)\n</code></pre> <pre><code>  &gt; 61\n</code></pre> <p>Setting up the Embedding Function</p> <pre><code>from langchain.embeddings import HuggingFaceBgeEmbeddings\n\nembedding_function = HuggingFaceBgeEmbeddings(\n    model_name=\"BAAI/bge-large-en-v1.5\",\n    model_kwargs={'device': 'cpu'},\n    encode_kwargs={\"normalize_embeddings\": True}\n)\n</code></pre> <p>Setting up the Vector store</p> <pre><code>from langchain.vectorstores import Chroma\n\ndb = Chroma.from_documents(docs, embedding_function, persist_directory=\"../output/human_rights\")\nbase_retriever = db.as_retriever()\n</code></pre> <p>Fixing a standard query for RAG</p> <pre><code>query = \"How does the declaration address the discrimination?\"\n</code></pre>"},{"location":"retrieval/#exercise-1","title":"Exercise 1","text":"<p>Building a RAG pipeline with Contextual Compression and Multi-query retrieval processes. And Perform Generation using Prompt Templates and Chat Models.</p> <ul> <li>Compressor -&gt; Contextual Compressor</li> <li>Retriever -&gt; Muti-query retreiver</li> </ul> <p>Compressor</p> <pre><code>from langchain.retrievers.document_compressors import LLMChainExtractor\n\nbase_compressor = LLMChainExtractor.from_llm(llm=chat)\n</code></pre> <p>Retriever</p> <pre><code>from langchain.retrievers.multi_query import MultiQueryRetriever\n\nmq_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=chat)\n</code></pre> <p>Compression Retriever</p> <pre><code>from langchain.retrievers import ContextualCompressionRetriever\n\ncompression_retriever = ContextualCompressionRetriever(base_compressor=base_compressor, base_retriever=mq_retriever)\n</code></pre> <p>Getting the matched documents</p> <pre><code>matched_docs = compression_retriever.get_relevant_documents(query=query)\n</code></pre> <p>Getting the text content from the matched docs</p> <pre><code>matched_content = \"\"\n\nfor doc in matched_docs:\n    page_content = doc.page_content\n    matched_content+=page_content\n    matched_content += \"\\n\\n\"\n</code></pre> <p>Performing Query Augmentation and Response Generation</p> <pre><code>from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n\ntemplate = \"\"\"\nAnswer the following question only by using the context given below in the triple backticks, do not use any other information to answer the question. If you can't answer the given question with the given context, you can return an emtpy string ('')\n\nContext: ```{context}```\n----------------------------\nQuestion: {query}\n----------------------------\nAnswer: \"\"\"\n\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(template=template)\nchat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\nprompt = chat_prompt.format_prompt(query=query, context=matched_content)\nresponse = chat(messages = prompt.to_messages()).content\n\nresponse\n</code></pre> <pre><code>  &gt; 'The declaration addresses discrimination by stating that everyone is entitled to all rights and freedoms without any distinction based on race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth, or other status. It also mentions that everyone has the right to equal pay for equal work without any discrimination.\n</code></pre>"},{"location":"retrieval/#exercise-2","title":"Exercise 2","text":"<p>In this exercise, let's combine multiple compressors and retrievers and build a RAG pipeline using Chains.</p> <ul> <li>Compressor -&gt; Custom Compressor + Contextual Compressor + redundant filter, reordering</li> <li>Retriever -&gt; Ensemble Retriver (Muti-query retreiver, BM25, Parent Document)</li> </ul> <p>Compressors</p> <pre><code># Imports\nfrom langchain.chains import HypotheticalDocumentEmbedder\nfrom langchain.document_transformers import EmbeddingsRedundantFilter\nfrom langchain_community.document_transformers import LongContextReorder\nfrom langchain.retrievers.document_compressors import DocumentCompressorPipeline\n\n# Filtering\nhyde_embedding_function = HypotheticalDocumentEmbedder.from_llm(llm=chat, base_embeddings=embedding_function, prompt_key=\"web_search\")\nredundant_filter = EmbeddingsRedundantFilter(embeddings=hyde_embedding_function)\n\n# Reordering\nlcr = LongContextReorder()\n\ncompression_pipeline = DocumentCompressorPipeline(transformers=[redundant_filter, lcr])\n\ncompression_pipeline\n</code></pre> <p>Individual Retrievers</p> <pre><code># Imports\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers import TFIDFRetriever, MultiQueryRetriever, ParentDocumentRetriever, EnsembleRetriever, ContextualCompressionRetriever\n\n## TFIDF\ntfidf_retriever = TFIDFRetriever.from_documents(docs)\n\n## Multi-Query Retriver\nmq_retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=chat)\n\n## Parent-Document Retriver\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)\nchild_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\nstore = InMemoryStore()\n\n### Creating an instance of parent document retriever\npar_doc_retriever = ParentDocumentRetriever(\n    vectorstore=db,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Adding documents to the Parent-Document Retriever\npar_doc_retriever.add_documents(docs)\n</code></pre> <p>Ensemble Retriver</p> <pre><code>retriever_pipeline = EnsembleRetriever(retrievers=[tfidf_retriever, mq_retriever, par_doc_retriever], weights=[0.4, 0.3, 0.3])\n\ncompression_retriever = ContextualCompressionRetriever(base_compressor=compression_pipeline, base_retriever=retriever_pipeline)\n\nmatched_docs = compression_retriever.get_relevant_documents(query=query)\n\n# matched_docs # Getting the matched documents\n</code></pre> <p>Creating an instance of Retrieval QA Chain for RAG</p> <pre><code>from langchain.chains import RetrievalQA\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=chat,\n    chain_type='stuff',\n    retriever=compression_retriever,\n    return_source_documents=True\n)\n</code></pre> <p>Checking the internal prompt created for Retrieval QA Chain</p> <pre><code>print(qa_chain.combine_documents_chain.llm_chain.prompt)\n</code></pre> <pre><code>  &gt; input_variables=['context', 'question'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]\n</code></pre> <p>Response Generation for the given Query</p> <pre><code>qa_chain(query)\n</code></pre> <pre><code>  &gt; {'query': 'How does the declaration address the discrimination?',\n  'result': 'The Universal Declaration of Human Rights addresses discrimination by stating that everyone is entitled to all the rights and freedoms set forth in the Declaration without any distinction of any kind, such as race, color, sex, language, religion, political or other opinion, national or social origin, property, birth, or other status. It ensures equal protection of the law and protection against any form of discrimination.',\n  ...\n  _DocumentWithState(page_content='| Universal Declaration of Human Rights | 16\\nArticleUNITED\\nNATIONS\\nAll are equal before the law and are \\nentitled  without any  discrimination \\nto equal protection of the law. All are entitled to  equal protection against any discrimination in violation of this \\nDeclaration and against any incitement \\nto such discrimination.', metadata={'source': '../datasets/udhr_booklet_en_web.pdf', 'page': 24}, state={'embedded_doc': [-0.05549198389053345, ..., -0.009328608401119709]})]}\n</code></pre>"},{"location":"setup/","title":"Project Setup","text":""},{"location":"setup/#virtual-environment-creation","title":"Virtual environment creation","text":""},{"location":"setup/#conda-environment","title":"Conda environment","text":"<p>Global environment</p> <pre><code>conda create -n env_name\n</code></pre> <p>Local environment (considering the local environment to be the current directory)</p> <pre><code>conda create -p ./env_name\n</code></pre>"},{"location":"setup/#python-virtual-environment","title":"Python virtual environment","text":"<pre><code>python -m venv env_name\n</code></pre> <p>Local environment</p> <pre><code>python -m venv ./env_name\n</code></pre>"},{"location":"setup/#activating-virtual-environment","title":"Activating virtual environment","text":""},{"location":"setup/#conda-environment_1","title":"Conda environment","text":"<p>Global environment</p> <pre><code>conda activate env_name\n</code></pre> <p>Local environment</p> <pre><code>conda activate ./env_name\n</code></pre>"},{"location":"setup/#python-virtual-environment_1","title":"Python virtual environment","text":"<pre><code>source env_name/Scripts/activate\n</code></pre> <p>Local environment</p> <pre><code>source ./env_name/Scripts/activate\n</code></pre>"},{"location":"setup/#required-packages","title":"Required packages","text":"<pre><code>ipykernel\nlangchain==0.0.351\nlangchain-community==0.0.4\nlangchain-core==0.1.1\nlangsmith==0.0.72\nopenai==1.6.0\npydantic==2.5.2\npydantic_core==2.14.5\ngradio==4.19.2\ngradio_client==0.10.1\nlangchain_openai==0.0.8\narxiv==2.1.0\npymupdf==1.23.26\nbeautifulsoup4==4.12.3\nlxml==4.9.4\nwikipedia==1.4.0\npypdf==4.0.2\nunstructured==0.12.5\ntiktoken==0.6.0\n# llama_cpp_python\n</code></pre>"},{"location":"setup/#install-requirementstxt-file","title":"Install requirements.txt file","text":"<p>Prepare the requirements.txt file with the above package list</p> <pre><code>pip install -r requirements.txt \n</code></pre> <p>If you are already having existing packages installed, use the below command for installing the exact versions</p> <pre><code>pip install --force-reinstall -v -r requirements.txt \n</code></pre>"}]}